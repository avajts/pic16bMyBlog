[
  {
    "objectID": "posts/Welcome/index.html",
    "href": "posts/Welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is my first post in a Quarto blog. Welcome!"
  },
  {
    "objectID": "posts/Spotify Collaborative Music Generator/index.html",
    "href": "posts/Spotify Collaborative Music Generator/index.html",
    "title": "Spotify Collaborative Playlist Generator",
    "section": "",
    "text": "Link to GitHub Repository\nThe GitHub repository for our project can be found here: https://github.com/avajts/pic16b_project.git\n\n\nOverview\nThe Spotify Collaborative Playlist Generator is a Flask-based web application designed to enhance the music discovery experience. By integrating with the Spotify API, the app retrieves a user’s playlists and tracks and leverages a K-Nearest Neighbors (KNN) machine learning model to recommend new songs. These recommendations are generated using a dataset of Spotify tracks from Kaggle, allowing users to explore music selections tailored to their preferences. In addition to personalized recommendations, the app provides interactive data visualizations to offer insights into listening habits and genre trends.\nOne of the standout features of this app is its collaborative functionality, allowing multiple users to log in simultaneously. The recommended playlist is based on the combined preferences of all logged-in users, ensuring a balanced mix of music that aligns with the group’s shared tastes. Users can easily set up the app by cloning the repository, installing Flask, and running the application in their terminal. Once authenticated with Spotify, they can generate custom playlists by selecting a genre and specifying the number of songs they’d like to add. Behind the scenes, the app efficiently manages user data using an SQLite database while employing data preprocessing and feature engineering to optimize the recommendation system. This project not only demonstrates how machine learning can enhance music discovery but also showcases practical applications of web development, API integration, and data analysis.\nTo build an effective recommendation system, we first analyzed the relationships between various song features, such as energy, danceability, and acousticness. The correlation matrix heatmap visually represents how different track features are related to each other, helping us identify patterns in the dataset that may influence song recommendations. The more positive a correlation is, the more strongly correlated the two features are, which implies we should use features like: * energy and loudness * valence and danceability * speechiness and explicit\nBy leveraging this insight, our KNN model can make more informed recommendations, ensuring that suggested songs align well with the user’s listening habits.\n\nfrom eda import plot_correlation_heatmap\nfrom music_rec import get_spotify_df\n\nspotify_df = get_spotify_df()\nplot_correlation_heatmap(spotify_df)\n\n\n\n\n\n\n\n\n\n\nTechnical Component 1: Dynamic Website\nThe Flask web app allows multiple users to log in with their Spotify accounts and see the names and artists of the songs in their Spotify playlists that are also in the Kaggle dataset. These are the songs that their recommended playlist will be based on, since they are the ones we have the song features for. From the /playlists page, users can then choose to log in with another Spotify account, see their recommended playlist, or see data visualizations on how each user’s song choices compare. If they choose to see visualizations, they can view a two-variable scatterplot or a genre histogram. To see their recommended playlist, they will be prompted to enter the genre and number of songs they want, and will then be shown a generated playlist meeting those specifications.\nBehind the scenes, this is how the web app works: when a user logs in with their Spotify account and grants the app permission to access their data, the app sends a request Spotify through the Spotify API to access it. Their song information is then stored (temporarily) in a SQL database. Each page of the app has a function that tells it what to do, like calling another function, displaying a table, and/or rendering an HTML file. For example, the /display-recommended page has a function display_recommended() that gets a Pandas DataFrame of user songs from a function that gets the table of user songs from the SQL database, prepares it, and returns it as a dataframe. It gets a dataframe of Spotify tracks from the Kaggle dataset similarly. Then it takes in the genre and n (number of songs) input from the form the user filled out, and passes those as parameters to the function recommend_songs(), which uses the KNN model to generate the recommended playlist that it returns. display_recommended() returns rendered HTML files to display the input form and the table of recommended songs.\n@app.route('/display-recommended', methods=['GET', 'POST'])\n\ndef display_recommended():\n\n    user_df = get_user_df()\n    df_spotify_tracks = get_spotify_df()\n\n    if request.method == 'POST':\n        genre = request.form.get('genre', None)\n        n = request.form.get('n', '5')\n\n        try:\n            n = int(n)  # Convert to integer\n        except ValueError:\n            return \"Error: n must be an integer.\", 400  # Handle invalid numbers\n\n        if n &lt;= 0:\n            return \"Error: n must be a positive integer.\", 400\n\n        # Generate the recommended playlist\n        recommended = recommend_songs(user_df, df_spotify_tracks, genre=genre, n=n)\n    \n        # Convert DataFrame to HTML\n        recommended_html = recommended.to_html(classes='table table-striped', index=False)\n    \n        return render_template('display_recommended.html', table=recommended_html)\n\n    return render_template('input_form.html')  \nHere is an example of what the /display-recommended page looks like showing a recommended playlist of 20 songs in the “pop” genre (only some are showing in the screenshot but in the app the user can scroll to see the rest of the songs):\n\n\n\nTechnical Component 2: Creating and Interacting with an SQL Database\nSince there is so much data involved in our app, we created an SQL database called spotify_dataset.db. First, we loaded the Kaggle dataset into this database under its own table called spotify_tracks. Then, when the app is run and the user logs in, their data is automatically loaded into the database in different tables called user_tracks and user_playlists, which contain the information from the Spotify API call (the Spotify API actually yields the data in the format of a json object, which we then load into a dataframe, which we then write into the database). Instead of creating a different table for each user, we opted to have one combined table for all the users and an extra username column which specifies which user that row corresponds to.\nAfter the user logs into our website, they are shown a table which displays the songs that overlap between their personal Spotify and our Kaggle dataset – we wanted the user to be able to see which of their songs we are using to generate our recommendations and visualizations. Because the API call yields the data as a json object, the information about the artists is stored in the database as one long string of the form:\n{\n  \"external_urls\": {\n    \"spotify\": \"string\"\n  },\n  \"followers\": {\n    \"href\": \"string\",\n    \"total\": 0\n  },\n  \"genres\": [\"Prog rock\", \"Grunge\"],\n  \"href\": \"string\",\n  \"id\": \"string\",\n  \"images\": [\n    {\n      \"url\": \"https://i.scdn.co/image/ab67616d00001e02ff9ca10b55ce82ae553c8228\",\n      \"height\": 300,\n      \"width\": 300\n    }\n  ],\n  \"name\": \"string\",\n  \"popularity\": 0,\n  \"type\": \"artist\",\n  \"uri\": \"string\"\n}\nIn order to display the songs and artists in a nice fashion, we extracted the artists’ names using the command:\n    display['track_artists'] = (display['track_artists'].str.findall(r'\\bname\": \"([^\"]*)')).apply(lambda x: str(x))\nHere is an example of what this display table might look like (of course, your songs would be your own!):\n\nWe also query the database in many other functions to obtain whatever specific data is needed.\nFinally, we created the function clear_database() to ensure that each new time the app starts, the database is emptied so that the users only have their data being used.\n\n\nTechnical Component 3: Interactive Data Visualization\nAfter logging into the website, users also have the option to view combined data. Clicking on this button will then prompt them to choose which type of data visualization they would like to see. As of now, there are two options: a two-variable scatterplot or a genre histogram.\nIf the user chooses two-variable scatterplot, they will be taken to the page /two-var-plot, which renders an html file. This will display the plot itself (the default is loudness vs. energy) and the option to customize the chart by changing which variables it plots. The plot is created by querying the SQL database and loading the data into a dataframe, then using plotly to make the scatterplot. If the user changes the variables by entering the variables they want and clicking the Submit button, the page automatically reloads and the chart is updated accordingly.\nIf the user chooses genre histogram, they will be taken to a page displaying a histogram of the genres they listen to. If there is more than one user logged in, it will create side-by-side charts, one for each user. The code for this page works in the same way: it queries the SQL database, makes a dataframe, and then uses plotly to create the chart out of that dataframe.\n@app.route('/display-data/two-var-plot', methods=['GET', 'POST'])\ndef display_two_var():\n    \n    user_df = get_user_df()\n\n    if request.method == 'POST':\n        features = [request.form.get('feature0', 'loudness'), request.form.get('feature1', 'energy')]\n        session['features'] = features  # Store in session\n        return redirect(url_for('display_two_var'))  # Redirect after POST\n    \n    features = session.get('features', ['loudness', 'energy'])  # Load stored features\n    fig = two_var_plot(user_df, features)\n    graph_json = json.dumps(fig, cls=utils.PlotlyJSONEncoder)\n    \n    return render_template('two_var_plot.html', graph_json=graph_json)\nHere is an example of the two-variable scatterplot (on the app, it would be interactive, with the colors corresponding to different users).\n\n\n\nTechnical Component 4: Machine Learning Model’s Performance\nThe K-Nearest Neighbors (KNN)-based recommendation system demonstrates a strong ability to suggest songs that align with user preferences when trained on genre-specific subsets rather than the entire dataset. By first filtering the Spotify dataset by genre and then applying KNN to find the most similar tracks based on audio features such as danceability, energy, and valence, the model ensures that recommendations align with the user’s listening patterns. This genre-specific approach helps reduce the risk of recommendations being skewed toward dominant characteristics in the full dataset, which was observed when training the model without genre constraints. However, one limitation lies in the dataset itself—genres in the Spotify dataset may not always be correctly labeled or well-represented, leading to recommendations that may not fully capture a user’s actual taste. Additionally, certain genres, particularly newer ones like indie and pop, have fewer data points available, which reduces the model’s ability to generate high-quality recommendations due to a lack of sufficient training examples.\nAnother important consideration is the model’s reliance on randomly selecting songs from the top recommendations. While randomness introduces diversity, it also increases the chance of selecting less relevant songs instead of the best possible matches. Furthermore, KNN’s performance heavily depends on the choice of distance metric (Euclidean distance in this case), which may not always capture musical similarity in an optimal way. A stronger approach might involve integrating cosine similarity or a hybrid recommendation system that combines content-based filtering with collaborative filtering. Lastly, the filtering mechanism ensures users are not recommended tracks they already have in their playlists, which enhances suggesting unfamiliar songs. Future improvements could involve refining the dataset, experimenting with feature weighting, or incorporating deep learning models for more specific recommendations.\ndef recommend_songs(user_songs, spotify_tracks, genre=None, n=5, random_state=42):\n    \"\"\"\n    Generate a personalized playlist of `n` songs from a specific genre based on the user's preferences.\n\n    Args:\n        df: The user's listening history with audio features.\n        genre: The genre of songs to recommend.\n        n: The number of songs to recommend (default is 5).\n        random_state: Seed for reproducibility (default is 42).\n\n    Returns:\n        random_recommendations: A DataFrame containing the recommended songs.\n    \"\"\"\n    # Filter the Spotify dataset to include only songs from the specified genre\n    genre_tracks = spotify_tracks[spotify_tracks['track_genre'].str.lower() == genre.lower()]\n\n    # Check if there are enough songs in the specified genre\n    if len(genre_tracks) &lt; n:\n        raise ValueError(f\"Not enough songs in the '{genre}' genre. Only {len(genre_tracks)} songs available.\")\n\n    # Select relevant features for the KNN model\n    features = ['danceability', 'energy', 'key', 'loudness', 'mode', 'speechiness',\n                'acousticness', 'instrumentalness', 'liveness', 'valence', 'tempo']\n\n    # Prepare the feature matrix for the user's listening history\n    X_user = user_songs[features]\n\n    # Train the KNN model on the user's data\n    # Standardize the features\n    scaler = StandardScaler()\n    X_user_scaled = scaler.fit_transform(X_user)\n\n    # Ensure `n` does not exceed the number of available songs in the dataset\n    n = min(n, len(X_user_scaled))\n\n    # Train the KNN model on the user's features\n    knn = NearestNeighbors(n_neighbors=n, metric='euclidean')  # Use Euclidean distance\n    knn.fit(X_user_scaled)\n\n    # Prepare the feature matrix for the genre-specific songs\n    X_genre = genre_tracks[features]\n    X_genre_scaled = scaler.transform(X_genre)\n\n    # Find the nearest neighbors (most similar songs) in the genre-specific dataset\n    distances, indices = knn.kneighbors(X_genre_scaled)\n\n    # Flatten the indices array to get a list of all recommended song indices\n    recommended_song_indices = indices.flatten()\n\n    # Ensure indices are within the valid range of the genre_tracks DataFrame\n    valid_indices = [idx for idx in recommended_song_indices if idx &lt; len(genre_tracks)]\n\n    if not valid_indices:\n        raise ValueError(\"No valid recommendations found. Please check the input data.\")\n\n    # Get the recommended songs\n    recommended_songs = genre_tracks.iloc[valid_indices].drop_duplicates(subset=['track_name', 'artists']).head(n)\n\n    # Randomly select `n` songs from the recommendations\n    random_recommendations = recommended_songs.sample(n=n, random_state=random_state)\n\n    random_recommendations = random_recommendations[['artists', 'track_name', 'track_genre']]\n\n    # Display the recommended songs\n    return random_recommendations\nThe genre count visualization highlights the distribution of different music genres in our dataset, which directly impacts the performance of our KNN recommendation model. Since some genres are significantly underrepresented, the model may struggle to provide accurate recommendations for those genres due to limited data. This imbalance reinforces the need for careful dataset preprocessing and genre-specific training to ensure diverse and relevant song suggestions.\n\nfrom eda import plot_genre_count\nplot_genre_count(spotify_df)\n\n\n\n\n\n\n\n\n\n\nConcluding Remarks\nSome ideas we have for future improvements include: * Enhancing dataset quality for better genre classification– as discussed in Techincal Component 3, there are certain limitations to the dataset we used. * Improving recommendation accuracy with deep learning models.\nWe hope this project will only contribute to users’ enjoyment of music. We do not see any ethical concerns, as we do not have access to users’ personal data in their Spotify accounts and no information is saved. Only their playlist information is stored in the SQL database for the duration of that instance of the app running, and the database is cleared each time the app runs."
  },
  {
    "objectID": "posts/Message Bank using Dash/index.html",
    "href": "posts/Message Bank using Dash/index.html",
    "title": "Simple Message Bank with Dash",
    "section": "",
    "text": "For this project, I built a simple message board web application that allows users to submit and view messages. The app stores messages in an SQLite database and displays them dynamically using Dash. In this post, I’ll walk through the key functions used to build this app, from database management to frontend design."
  },
  {
    "objectID": "posts/Message Bank using Dash/index.html#introduction",
    "href": "posts/Message Bank using Dash/index.html#introduction",
    "title": "Simple Message Bank with Dash",
    "section": "",
    "text": "For this project, I built a simple message board web application that allows users to submit and view messages. The app stores messages in an SQLite database and displays them dynamically using Dash. In this post, I’ll walk through the key functions used to build this app, from database management to frontend design."
  },
  {
    "objectID": "posts/Message Bank using Dash/index.html#setting-up-the-database",
    "href": "posts/Message Bank using Dash/index.html#setting-up-the-database",
    "title": "Simple Message Bank with Dash",
    "section": "Setting Up the Database",
    "text": "Setting Up the Database\nThe first step in constructing the app was to set up a database to store messages. I created a function called get_message_db() to handle the database connection and ensure that the required table exists.\nfrom dash import Dash, html, dash_table, dcc, callback, Output, Input, State\nimport pandas as pd\nimport plotly.express as px\nimport dash_bootstrap_components as dbc\nimport sqlite3\n\nmessage_db = None\n\ndef get_message_db():\n    \"\"\"\n    Creates the database of messages.\n    \"\"\"\n    global message_db # connect to existing message_db on a global scale\n    if message_db is not None:\n        try:\n            message_db.cursor()  # Test if the connection is still open\n            return message_db\n        except sqlite3.ProgrammingError:\n            message_db = None  # Reset if the connection was closed\n\n    # Open a new connection if needed\n    message_db = sqlite3.connect(\"messages_db.sqlite\", check_same_thread=False)\n    cmd = '''\n        CREATE TABLE IF NOT EXISTS messages(\n            handle TEXT,\n            message TEXT\n        )''' \n    cursor = message_db.cursor()\n    cursor.execute(cmd)\n    return message_db\nThis function ensures that only one connection to the database exists globally. If the table doesn’t exist, it creates one with handle and message columns."
  },
  {
    "objectID": "posts/Message Bank using Dash/index.html#inserting-messages",
    "href": "posts/Message Bank using Dash/index.html#inserting-messages",
    "title": "Simple Message Bank with Dash",
    "section": "Inserting Messages",
    "text": "Inserting Messages\nTo allow users to submit messages, I wrote the insert_message() function:\ndef insert_message(handle, message):\n    \"\"\"\n    Handles inserting a user message into the database of messages.\n    \"\"\"\n    # Connect to db by called our previous function\n    message_db = get_message_db()\n    cursor = message_db.cursor()\n    # Insert the handle and message into the database\n    cursor.execute(\"INSERT INTO messages VALUES (?,?)\",(handle,message))\n    # Commit and close the db\n    message_db.commit()\n    message_db.close()\nThis function takes in a user’s handle and message and inserts it into the database. The commit() ensures that the changes are saved."
  },
  {
    "objectID": "posts/Message Bank using Dash/index.html#retrieving-messages",
    "href": "posts/Message Bank using Dash/index.html#retrieving-messages",
    "title": "Simple Message Bank with Dash",
    "section": "Retrieving Messages",
    "text": "Retrieving Messages\nTo display messages, I implemented the random_messages() function:\ndef random_messages(n):\n    \"\"\"\n    Returns a collection of n random messages from the 'message_db', or fewer if necessary.\n    \"\"\"\n\n    message_db = get_message_db()\n    cursor = message_db.cursor()\n    # Count how many messages are in db\n    query = f\"Select COUNT(*) FROM messages\"\n    cursor.execute(query)\n    result = cursor.fetchone()\n    row_count = result[0]\n\n    # If there are n many messages or more\n    if row_count &gt;= n:\n        # Randomly select n messages from db\n        cursor.execute(f\"SELECT * FROM messages ORDER BY RANDOM() LIMIT {n}\")\n    else:\n        # Randomly select all messages that are less than n\n        cursor.execute(f\"SELECT * FROM messages ORDER BY RANDOM()\")\n    \n    rows = cursor.fetchall()\n    message_db.close()\n    return rows \nThis function selects a random subset of messages from the database, allowing for dynamic updates each time the user refreshes the page."
  },
  {
    "objectID": "posts/Message Bank using Dash/index.html#submitting-messages",
    "href": "posts/Message Bank using Dash/index.html#submitting-messages",
    "title": "Simple Message Bank with Dash",
    "section": "Submitting Messages",
    "text": "Submitting Messages\nThe frontend component for submitting messages uses Dash callbacks. The submit() function handles form submissions:\n@callback(\n    Output('post-submit-text', 'children'),\n    Input('submit-button', 'n_clicks'),\n    [State('handle-input-box', 'value'), \n    State('message-input-box', 'value')],\n    prevent_initial_call=True\n)\ndef submit(n_clicks, handle, message):\n    \"\"\"\n    Callback function to update the components.\n    \"\"\"\n\n    # Both entry boxes have values, and button has been clicked at least once\n    if n_clicks &gt; 0 and handle and message: \n        # Insert our message to databse by calling our previous function\n        insert_message(handle, message)\n        return 'Thank you for submitting a message!'\n    # If user doesn't enter message or handle value\n    else:\n        return 'Error when submitting, please enter text and click submit.'\nThis function checks if both fields are filled before calling insert_message(), ensuring valid user input."
  },
  {
    "objectID": "posts/Message Bank using Dash/index.html#viewing-messages",
    "href": "posts/Message Bank using Dash/index.html#viewing-messages",
    "title": "Simple Message Bank with Dash",
    "section": "Viewing Messages",
    "text": "Viewing Messages\nTo display messages, I wrote the view() function, and enhanced it’s styling for to improve user interface:\n@callback(\n    Output('random-messages-output', 'children'),\n    Input('update-button', 'n_clicks'),\n    prevent_initial_call=True\n)\ndef view(n_clicks):\n    \"\"\"\n    Callback function to display random messages.\n    \"\"\"\n    # Get up to 5 random messages\n    messages = random_messages(5)\n\n    if not messages:\n        return \"No messages in the database yet.\"\n\n    # Format messages for display\n    formatted_messages = [\n        html.P(\n            [\n                html.Span(msg, style={\"font-weight\": \"bold\", \"color\": \"#2E86C1\"}),  # Message in bold blue\n                html.Br(),\n                html.Span(f\"- {handle}\", style={\"font-style\": \"italic\", \"color\": \"#A93226\"})  # Handle in italic red\n            ],\n            style={\n                \"font-family\": \"'Courier New', monospace\",  # Monospace font for readability\n                \"border\": \"1px solid #ddd\",  # Light border\n                \"border-radius\": \"8px\",\n                \"padding\": \"10px\",\n                \"margin-bottom\": \"10px\",\n                \"background-color\": \"#F8F9F9\",  # Light gray background\n                \"box-shadow\": \"2px 2px 5px rgba(0, 0, 0, 0.1)\"  # Soft shadow for depth\n            }\n        ) \n        for handle, msg in messages\n    ]\n    \n    return formatted_messages\nThis function fetches and formats messages using a combination of Python string manipulation and Dash’s HTML components."
  },
  {
    "objectID": "posts/Message Bank using Dash/index.html#app-layout",
    "href": "posts/Message Bank using Dash/index.html#app-layout",
    "title": "Simple Message Bank with Dash",
    "section": "App Layout",
    "text": "App Layout\nFor the app’s layout, it was built with a total of eleven user interface elements. I enhanced the style of the message and handle input boxes, as well as the submit and view boxes.\napp = Dash()\n\napp.layout = html.Div([\n    html.Div(children='SIMPLE MESSAGE BANK', \n             style={'font-family': 'Arial', 'font-size': '40px'\n                   }),\n    html.Div(children='Submit', \n             style={'font-family': 'Arial', 'font-size': '30px',\n                    'position': 'absolute', 'top': '60px', 'left': '7px'\n                   }),\n    html.Div(children='Your Message:',\n             style={'font-family': 'Arial', 'font-size': '20px',\n                   'position': 'absolute', 'top': '103px', 'left': '7px'\n                   }),\n    dcc.Input(id=\"message-input-box\",\n        type=\"text\",\n        placeholder=\"Type your message...\",\n        style={\"width\": \"100%\", \"padding\": \"10px\", \n                'position': 'absolute', 'top': '130px',\n                \"border\": \"2px solid #3498db\",  # Blue border\n                \"border-radius\": \"8px\", \"font-size\": \"16px\",\n                \"font-family\": \"'Arial', sans-serif\", \"color\": \"#333\",\n                \"background-color\": \"#f8f9f9\",  # Light gray background\n                \"outline\": \"none\",\n        }),\n    html.Div(children='Your Name or Handle:',\n             style={'font-family': 'Arial', 'font-size': '20px',\n                       'position': 'absolute', 'top': '177px', 'left': '7px'}),\n    dcc.Input(id=\"handle-input-box\",\n        type=\"text\",\n        placeholder=\"Your handle...\",\n        style={\"width\": \"100%\", \"padding\": \"10px\", \n                'position': 'absolute', 'top': '203px',\n                \"border\": \"2px solid #A93226\",  # Red border\n                \"border-radius\": \"8px\", \"font-size\": \"16px\",\n                \"font-family\": \"'Arial', sans-serif\", \"color\": \"#333\",\n                \"background-color\": \"#f8f9f9\", \"outline\": \"none\",\n        }),\n    html.Button('Submit', \n                id='submit-button',\n                n_clicks=0,\n                style={'position': 'absolute', 'top': '250px', 'left': '7px',\n                       \"background-color\": \"#28a745\",  # Green color\n                        \"color\": \"white\", \"border\": \"none\",\n                        \"padding\": \"12px 20px\", \"border-radius\": \"8px\",\n                        \"font-size\": \"16px\", \"cursor\": \"pointer\",\n                        \"transition\": \"0.3s\", \"font-family\": \"'Arial', sans-serif\"\n                      }),\n    html.Div(id='post-submit-text',\n            style={'position': 'absolute', 'top': '300px', 'left': '7px', \n                    'font-family': 'Arial', 'font-size': '16px'\n                  }),\n    html.Div(children='View', id='view-text',\n             style={'position': 'absolute', 'top': '325px', 'left': '7px', \n                    'font-family': 'Arial', 'font-size': '30px'\n                  }),\n    html.Button('Update',\n                id='update-button',\n                n_clicks=0,\n                style={'position': 'absolute', 'top': '325px', 'left': '100px',\n                        \"background-color\": \"#007bff\",  # Blue color\n                        \"color\": \"white\", \"border\": \"none\",\n                        \"padding\": \"12px 20px\", \"border-radius\": \"8px\",\n                        \"font-size\": \"16px\", \"cursor\": \"pointer\",\n                        \"transition\": \"0.3s\", \"font-family\": \"'Arial', sans-serif\",\n                        }),\n    html.Div(id='random-messages-output',\n             style={'position': 'absolute', 'top': '360px', 'left': '7px',\n                    'font-family': 'Arial', 'font-size': '16px'          \n                     })\n])\nThese styles improve readability and interactivity by changing the colors and adding borders to the elements."
  },
  {
    "objectID": "posts/Message Bank using Dash/index.html#screenshots",
    "href": "posts/Message Bank using Dash/index.html#screenshots",
    "title": "Simple Message Bank with Dash",
    "section": "Screenshots",
    "text": "Screenshots\nBelow are screenshots demonstrating message submission and viewing messages in the message bank. In the first image, I’ve entered in my message and name, then pressed submit. In the second image, I’ve pressed view, allowing me to see up to five messages in the database (but it currently contains two)."
  },
  {
    "objectID": "posts/Message Bank using Dash/index.html#conclusion",
    "href": "posts/Message Bank using Dash/index.html#conclusion",
    "title": "Simple Message Bank with Dash",
    "section": "Conclusion",
    "text": "Conclusion\nThis project demonstrated how to build a simple web app using Dash and SQLite to store and display user-submitted messages. Through well-structured Python functions and frontend styling, the app provides an engaging experience for users to interact with."
  },
  {
    "objectID": "posts/Climate Database Visualizations/index.html",
    "href": "posts/Climate Database Visualizations/index.html",
    "title": "Climate Database Visualizations",
    "section": "",
    "text": "Using the data for temperatures and the stations they were recorded at, along with this dataset of countries, I decided to create a few visualizations depicting the temperature and its fluctions based on geography and change in time.\n\n\n\nI started by importing the necessary libraries. I then read in all the data necessary from recording temperature and their stations data by creating a file ‘datafiles’, and reading in a decade of information at a time as separate csv files.\n\nimport pandas as pd\nimport numpy as np\nimport sqlite3\nimport plotly.io as pio\npio.renderers.default=\"iframe\"\n\nimport os\n# create folder named \"datafiles\" if it does not exist\nif not os.path.exists(\"datafiles\"): \n    os.mkdir(\"datafiles\")\n\n# download the files\nimport urllib.request\nintervals = [f\"{i}-{i+9}\" for i in range(1901, 2020, 10)]\nfor interval in intervals:\n    url = f\"https://raw.githubusercontent.com/PIC16B-ucla/24F/main/datasets/noaa-ghcn/decades/{interval}.csv\"\n    urllib.request.urlretrieve(url, f\"datafiles/{interval}.csv\")\n\n\n\n\nNow I create a database in the current directory called “temps.db”, which will then be connected using SQL. This is necessary if we want to execute any further SQL commands on “temps.db”.\n\nconn = sqlite3.connect(\"temps.db\") # create a database in current directory called temps.db\n\n\n\n\nNext I created a function to transform a wide formated dataframe into a long formated dataframe. This definition also cleans up the Month and Temp columns and drops and NaN values from the Temp column.\n\ndef prepare_df(df):\n    \"\"\"\n    prepares a piece of wide format dataframe into a long format data frame\n    \"\"\"\n    # melt to the long format table\n    df = df.melt(\n        id_vars = [\"ID\", \"Year\"],\n        value_vars = [f\"VALUE{i}\" for i in range(1, 13)],\n        var_name = \"Month\",\n        value_name = \"Temp\"\n    )\n\n    # cleaning month and temp\n    df[\"Month\"] = df[\"Month\"].str[5:].astype(int)\n    df[\"Temp\"]  = df[\"Temp\"] / 100\n    df = df[~np.isnan(df[\"Temp\"])]\n\n    return df\n\n\n\n\nFirst we add in the ‘temperatures’ table using our established connection to temps.db, reading it in chunks because it is such a large databse.\n\nintervals = [f\"{i}-{i+9}\" for i in range(1901, 2020, 10)]\nfor i, interval in enumerate(intervals):\n    filepath = f\"datafiles/{interval}.csv\"\n    df = pd.read_csv(filepath)\n    df = prepare_df(df)\n    df.to_sql(\"temperatures\", conn, if_exists = \"replace\" if i == 0 else \"append\", index = False)\n\nNext we add in the ‘stations’ and ‘countries’ table, only needing to use the raw csv link for both since they are smaller databases.\n\nfilename = \"https://raw.githubusercontent.com/PIC16B-ucla/25W/refs/heads/main/datasets/noaa-ghcn/station-metadata.csv\"\nstations = pd.read_csv(filename)\nstations.to_sql(\"stations\", conn, if_exists = \"replace\", index=False)\n\nfilename = \"https://raw.githubusercontent.com/mysociety/gaze/master/data/fips-10-4-to-iso-country-codes.csv\"\ncountries = pd.read_csv(filename)\ncountries.to_sql(\"countries\", conn, if_exists = \"replace\", index=False)\n\n279\n\n\nWe can now check to see what tables are contained within our connected database, and print out their correlating features.\n\ncursor = conn.cursor()\ncursor.execute(\"SELECT sql FROM sqlite_master WHERE type='table';\")\n\nfor result in cursor.fetchall():\n    print(result[0])\n\nCREATE TABLE \"temperatures\" (\n\"ID\" TEXT,\n  \"Year\" INTEGER,\n  \"Month\" INTEGER,\n  \"Temp\" REAL\n)\nCREATE TABLE \"stations\" (\n\"ID\" TEXT,\n  \"LATITUDE\" REAL,\n  \"LONGITUDE\" REAL,\n  \"STNELEV\" REAL,\n  \"NAME\" TEXT\n)\nCREATE TABLE \"countries\" (\n\"FIPS 10-4\" TEXT,\n  \"ISO 3166\" TEXT,\n  \"Name\" TEXT\n)"
  },
  {
    "objectID": "posts/Climate Database Visualizations/index.html#introduction",
    "href": "posts/Climate Database Visualizations/index.html#introduction",
    "title": "Climate Database Visualizations",
    "section": "",
    "text": "Using the data for temperatures and the stations they were recorded at, along with this dataset of countries, I decided to create a few visualizations depicting the temperature and its fluctions based on geography and change in time."
  },
  {
    "objectID": "posts/Climate Database Visualizations/index.html#importing",
    "href": "posts/Climate Database Visualizations/index.html#importing",
    "title": "Climate Database Visualizations",
    "section": "",
    "text": "I started by importing the necessary libraries. I then read in all the data necessary from recording temperature and their stations data by creating a file ‘datafiles’, and reading in a decade of information at a time as separate csv files.\n\nimport pandas as pd\nimport numpy as np\nimport sqlite3\nimport plotly.io as pio\npio.renderers.default=\"iframe\"\n\nimport os\n# create folder named \"datafiles\" if it does not exist\nif not os.path.exists(\"datafiles\"): \n    os.mkdir(\"datafiles\")\n\n# download the files\nimport urllib.request\nintervals = [f\"{i}-{i+9}\" for i in range(1901, 2020, 10)]\nfor interval in intervals:\n    url = f\"https://raw.githubusercontent.com/PIC16B-ucla/24F/main/datasets/noaa-ghcn/decades/{interval}.csv\"\n    urllib.request.urlretrieve(url, f\"datafiles/{interval}.csv\")"
  },
  {
    "objectID": "posts/Climate Database Visualizations/index.html#creating-a-sql-connection",
    "href": "posts/Climate Database Visualizations/index.html#creating-a-sql-connection",
    "title": "Climate Database Visualizations",
    "section": "",
    "text": "Now I create a database in the current directory called “temps.db”, which will then be connected using SQL. This is necessary if we want to execute any further SQL commands on “temps.db”.\n\nconn = sqlite3.connect(\"temps.db\") # create a database in current directory called temps.db"
  },
  {
    "objectID": "posts/Climate Database Visualizations/index.html#preparing-the-dataframe",
    "href": "posts/Climate Database Visualizations/index.html#preparing-the-dataframe",
    "title": "Climate Database Visualizations",
    "section": "",
    "text": "Next I created a function to transform a wide formated dataframe into a long formated dataframe. This definition also cleans up the Month and Temp columns and drops and NaN values from the Temp column.\n\ndef prepare_df(df):\n    \"\"\"\n    prepares a piece of wide format dataframe into a long format data frame\n    \"\"\"\n    # melt to the long format table\n    df = df.melt(\n        id_vars = [\"ID\", \"Year\"],\n        value_vars = [f\"VALUE{i}\" for i in range(1, 13)],\n        var_name = \"Month\",\n        value_name = \"Temp\"\n    )\n\n    # cleaning month and temp\n    df[\"Month\"] = df[\"Month\"].str[5:].astype(int)\n    df[\"Temp\"]  = df[\"Temp\"] / 100\n    df = df[~np.isnan(df[\"Temp\"])]\n\n    return df"
  },
  {
    "objectID": "posts/Climate Database Visualizations/index.html#adding-tables",
    "href": "posts/Climate Database Visualizations/index.html#adding-tables",
    "title": "Climate Database Visualizations",
    "section": "",
    "text": "First we add in the ‘temperatures’ table using our established connection to temps.db, reading it in chunks because it is such a large databse.\n\nintervals = [f\"{i}-{i+9}\" for i in range(1901, 2020, 10)]\nfor i, interval in enumerate(intervals):\n    filepath = f\"datafiles/{interval}.csv\"\n    df = pd.read_csv(filepath)\n    df = prepare_df(df)\n    df.to_sql(\"temperatures\", conn, if_exists = \"replace\" if i == 0 else \"append\", index = False)\n\nNext we add in the ‘stations’ and ‘countries’ table, only needing to use the raw csv link for both since they are smaller databases.\n\nfilename = \"https://raw.githubusercontent.com/PIC16B-ucla/25W/refs/heads/main/datasets/noaa-ghcn/station-metadata.csv\"\nstations = pd.read_csv(filename)\nstations.to_sql(\"stations\", conn, if_exists = \"replace\", index=False)\n\nfilename = \"https://raw.githubusercontent.com/mysociety/gaze/master/data/fips-10-4-to-iso-country-codes.csv\"\ncountries = pd.read_csv(filename)\ncountries.to_sql(\"countries\", conn, if_exists = \"replace\", index=False)\n\n279\n\n\nWe can now check to see what tables are contained within our connected database, and print out their correlating features.\n\ncursor = conn.cursor()\ncursor.execute(\"SELECT sql FROM sqlite_master WHERE type='table';\")\n\nfor result in cursor.fetchall():\n    print(result[0])\n\nCREATE TABLE \"temperatures\" (\n\"ID\" TEXT,\n  \"Year\" INTEGER,\n  \"Month\" INTEGER,\n  \"Temp\" REAL\n)\nCREATE TABLE \"stations\" (\n\"ID\" TEXT,\n  \"LATITUDE\" REAL,\n  \"LONGITUDE\" REAL,\n  \"STNELEV\" REAL,\n  \"NAME\" TEXT\n)\nCREATE TABLE \"countries\" (\n\"FIPS 10-4\" TEXT,\n  \"ISO 3166\" TEXT,\n  \"Name\" TEXT\n)"
  },
  {
    "objectID": "posts/Climate Database Visualizations/index.html#importing-functions",
    "href": "posts/Climate Database Visualizations/index.html#importing-functions",
    "title": "Climate Database Visualizations",
    "section": "Importing Functions",
    "text": "Importing Functions\nIn a separate file called ‘climate_database.py’, I wrote multiple functions which returns the data wrangling and visualizations seen in this blog post. The first one to be imported is ‘query_climate_databse’:\n\nfrom climate_database import query_climate_database\nimport inspect\nprint(inspect.getsource(query_climate_database))\n\ndef query_climate_database(db_file, country, year_begin, year_end, month):\n    \"\"\"\n    Arguments:\n        db_file = the file name for the database\n        country = a string giving the name of a country \n        year_begin = int giving the earliest year (inclusive)\n        year_end = int giving the latest year (inclusive)\n        month = int giving the month of the year\n    Return Value:\n        The function returns a pandas dataframe which contains the temperature readings for the specified country, \n        in the specified date range, in the specified month of the year. The dataframe will have the following columns: \n        NAME = The station's name\n        LATITUDE = The latitude of the station \n        LONGITUDE = The longitude of the station \n        Country = The name of the country in which ithe station is located\n        Year = The year in which the reading was taken\n        Month = The month in which the reading was taken\n        Temp = The average temperature at the specified station during the specified year and month\n    \"\"\"\n\n    cmd = \\\n        f\"\"\"\n        SELECT S.NAME, S.LATITUDE, S.LONGITUDE, C.\"Name\" AS Country, T.Year, T.Month, T.Temp\n        FROM temperatures T\n        JOIN stations S ON T.ID = S.ID\n        JOIN countries C ON substr(S.ID, 1, 2) = C.\"FIPS 10-4\"\n        WHERE C.Name = '{country}'\n        AND T.Year BETWEEN {year_begin} AND {year_end}\n        AND T.Month = {month}\n        ORDER BY S.NAME ASC, T.Year ASC;\n        \"\"\"\n    \n    with sqlite3.connect(f\"{db_file}\") as conn:\n        df = pd.read_sql_query(cmd, conn)\n        \n    return df"
  },
  {
    "objectID": "posts/Climate Database Visualizations/index.html#function-output",
    "href": "posts/Climate Database Visualizations/index.html#function-output",
    "title": "Climate Database Visualizations",
    "section": "Function Output",
    "text": "Function Output\n‘query_climate_database’ is used to create a dataframe using sql and the five input paramters. It joins all three tables contained within ‘temps.db’ together using INNER JOIN, and sorts the stations and years by ascending order.\n\nquery_climate_database(db_file = \"temps.db\",\n                      country = \"India\",\n                      year_begin = 1980,\n                      year_end = 2020,\n                      month = 1)\n\n\n\n\n\n\n\n\nNAME\nLATITUDE\nLONGITUDE\nCountry\nYear\nMonth\nTemp\n\n\n\n\n0\nAGARTALA\n23.883\n91.250\nIndia\n1980\n1\n18.21\n\n\n1\nAGARTALA\n23.883\n91.250\nIndia\n1981\n1\n18.25\n\n\n2\nAGARTALA\n23.883\n91.250\nIndia\n1982\n1\n19.31\n\n\n3\nAGARTALA\n23.883\n91.250\nIndia\n1985\n1\n19.25\n\n\n4\nAGARTALA\n23.883\n91.250\nIndia\n1988\n1\n19.54\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n3147\nVISHAKHAPATNAM\n17.717\n83.233\nIndia\n2016\n1\n25.09\n\n\n3148\nVISHAKHAPATNAM\n17.717\n83.233\nIndia\n2017\n1\n23.90\n\n\n3149\nVISHAKHAPATNAM\n17.717\n83.233\nIndia\n2018\n1\n22.65\n\n\n3150\nVISHAKHAPATNAM\n17.717\n83.233\nIndia\n2019\n1\n22.20\n\n\n3151\nVISHAKHAPATNAM\n17.717\n83.233\nIndia\n2020\n1\n23.75\n\n\n\n\n3152 rows × 7 columns"
  },
  {
    "objectID": "posts/Climate Database Visualizations/index.html#big-question",
    "href": "posts/Climate Database Visualizations/index.html#big-question",
    "title": "Climate Database Visualizations",
    "section": "Big Question",
    "text": "Big Question\nWe were asked to create a visualization the addresses the following question: &gt; How does the average yearly change in temperature vary within a givin country? The best way to answer this would be to create a visualization for the given country, showing this yearly change in temperature."
  },
  {
    "objectID": "posts/Climate Database Visualizations/index.html#importing-functions-1",
    "href": "posts/Climate Database Visualizations/index.html#importing-functions-1",
    "title": "Climate Database Visualizations",
    "section": "Importing Functions",
    "text": "Importing Functions\nIn ‘temperature_coefficient_plot’, I create a dataframe using our previously defined function, ‘query_climate_database’ on the given input parameters. Next, I group the dataframe by station, then filters out the stations that don’t have enough observations.\nI created a function ‘compute_trend’ within this function, which computes the temperature trend (slope) per station. It uses linear regression’s model coefficient from sklearn’s library as the return value for the slope. We group the filtered dataframe by name, latitude, and longitude, then apply the ‘compute_trend’ function.\nLast is to create the figure, which is done using plotly’s scatter_mapbox function. I added a colorbar to the figure and updated the margin for formatting.\n\nimport plotly.express as px\nfrom sklearn.linear_model import LinearRegression\n\nfrom climate_database import temperature_coefficient_plot\nprint(inspect.getsource(temperature_coefficient_plot))\n\ndef temperature_coefficient_plot(db_file, country, year_begin, year_end, month, min_obs=10, **kwargs):\n    \"\"\"\n    Arguments:\n        db_file = the file name for the database\n        country = a string giving the name of a country \n        year_begin = int giving the earliest year (inclusive)\n        year_end = int giving the latest year (inclusive)\n        month = int giving the month of the year\n    Return Value:\n        This function returns an interactive geographic scatterplot with a point for each station. \n        The color for each point reflects the yearly change in temperature during the specified month and time period at that station.\n    \"\"\"\n\n    # Get our specific df given input params using our previously defined function\n    df = query_climate_database(db_file, country, year_begin, year_end, month)\n\n    # Group by station and filter stations with sufficient observations\n    station_counts = df.groupby(\"NAME\")[\"Year\"].nunique()\n    valid_stations = station_counts[station_counts &gt;= min_obs].index\n    df_filtered = df[df[\"NAME\"].isin(valid_stations)]\n\n    # Function to compute temperature trend (slope) per station\n    def compute_trend(group):\n        if len(group) &lt; min_obs:\n            return np.nan  # Skip stations with insufficient data\n\n        X = group[\"Year\"].values.reshape(-1, 1) \n        y = group[\"Temp\"].values\n\n        model = LinearRegression()\n        model.fit(X, y)\n        return model.coef_[0]  # Extract slope\n\n    # Compute slope per station\n    trends = df_filtered.groupby([\"NAME\", \"LATITUDE\", \"LONGITUDE\"]).apply(compute_trend).reset_index(name=\"Trend\")\n\n    # Drop stations with NaN trends\n    trends = trends.dropna()\n    \n    fig = px.scatter_mapbox(trends,\n                            lat = \"LATITUDE\",\n                            lon = \"LONGITUDE\",\n                            hover_name = \"NAME\",\n                            color = \"Trend\",\n                            range_color = [-0.1, 0.1], \n                            zoom = 2,\n                            height = 300,\n                            hover_data = {\"Trend\": \":.4f\"}, # SHpws trend rounded to 2 decimals\n                            color_continuous_scale = kwargs.get(\"color_continuous_scale\", px.colors.diverging.RdGy_r),\n                            mapbox_style = kwargs.get(\"mapbox_style\", \"carto-positron\"),\n                            title = f\"Estimates of Yearly Increase in Temperature in Month {month} for Stations in {country}, years {year_begin}-{year_end}\"\n                           )\n\n    # Center colorbar at 0 \n    fig.update_layout(\n        coloraxis_colorbar = dict(\n            title = \"Estimated Yearly Increase (°C)\", \n            tickvals=[-0.1, -0.05, 0, 0.05, 0.1],  # Custom tick values\n            ticktext=[\"-0.1\", \"-0.05\", \"0\", \"0.05\", \"0.1\"]  # Custom tick labels\n        ))\n    \n    fig.update_layout(margin={\"r\":50,\"t\":50,\"l\":50,\"b\":0})\n    \n    return fig\n\n\n\n\nScatterplot Output\nNow we can create scatterplot of any country, year interval, and month to see its yearly increase in temperature on an interactive plot. Here is India iin January from 1980-2020:\n\ncolor_map = px.colors.diverging.RdGy_r # choose a colormap\n\nfig = temperature_coefficient_plot(\"temps.db\", \"India\", 1980, 2020, 1, \n                                   min_obs = 10, \n                                   zoom = 2,\n                                   mapbox_style=\"carto-positron\",\n                                   color_continuous_scale=color_map)\n\nfig.show()\n\n\n\n\nAlternatively, here is China in January from 1950-1990:\n\nfig = temperature_coefficient_plot(\"temps.db\", \"China\", 1950, 1990, 1, \n                                   min_obs = 10, \n                                   zoom = 2,\n                                   mapbox_style=\"carto-positron\",\n                                   color_continuous_scale=color_map)\n\nfig.show()"
  },
  {
    "objectID": "posts/Climate Database Visualizations/index.html#first-visualization",
    "href": "posts/Climate Database Visualizations/index.html#first-visualization",
    "title": "Climate Database Visualizations",
    "section": "First Visualization",
    "text": "First Visualization\nAfter thinking on our big question more, I thought of two new ways we could visualize temperature change yearly in a given country. The first is with a line plot showing the average temperature over time in a country for the given time interval. When looking at this visualization, we can compare warming/cooling trends in different parts of the country."
  },
  {
    "objectID": "posts/Climate Database Visualizations/index.html#importing-functions-2",
    "href": "posts/Climate Database Visualizations/index.html#importing-functions-2",
    "title": "Climate Database Visualizations",
    "section": "Importing Functions",
    "text": "Importing Functions\nFirst, I created ‘query_avg_temp_over_time’, which creates a database similar to ‘query_climate_database’ but instead only returns name, latitude, and the average temperature for each year.\n\nfrom climate_database import query_avg_temp_over_time\nprint(inspect.getsource(query_avg_temp_over_time))\n\ndef query_avg_temp_over_time(db_file, country, year_begin, year_end, month):\n    \"\"\"\n    Returns the average temperature for each year in a given country as a dataframe.\n    \"\"\"\n    cmd = f\"\"\"\n        SELECT T.Year, S.LATITUDE, AVG(T.Temp) AS AvgTemp\n        FROM temperatures T\n        JOIN stations S ON T.ID = S.ID\n        JOIN countries C ON substr(S.ID, 1, 2) = C.\"FIPS 10-4\"\n        WHERE C.Name = '{country}'\n        AND T.Year BETWEEN {year_begin} AND {year_end}\n        AND T.Month = {month}\n        GROUP BY T.Year, S.LATITUDE\n        ORDER BY T.Year ASC;\n    \"\"\"\n    with sqlite3.connect(f\"{db_file}\") as conn:\n        df = pd.read_sql_query(cmd, conn)\n        \n    return df\n\n\n\nNext, I created ‘plot_avg_temp_over_time’ which creates a line plot with two facets for the North and South region of the given country. First, I creat a dataframe using ‘query_avg_temp_over_time’, then define a region column in the dataframe based on the latitude’s median. Last is to use plot;y’s line function to create our visualization.\n\nfrom climate_database import plot_avg_temp_over_time\nprint(inspect.getsource(plot_avg_temp_over_time))\n\ndef plot_avg_temp_over_time(db_file, country, year_begin, year_end, month):\n    df = query_avg_temp_over_time(db_file, country, year_begin, year_end, month)\n\n    # Define \"north\" and \"south\" based on latitude\n    df[\"Region\"] = df[\"LATITUDE\"].apply(lambda x: \"North\" if x &gt;= df[\"LATITUDE\"].median() else \"South\")\n\n    # Get the yearly data for long term changes instead of seasonal\n    df_yearly = df.groupby([\"Year\", \"Region\"])[\"AvgTemp\"].mean().reset_index()\n\n    fig = px.line(df_yearly, \n                  x = \"Year\", \n                  y = \"AvgTemp\", \n                  color = \"Region\", \n                  title = f\"Average Temperature Over Time in {country} ({year_begin}-{year_end})\", \n                  labels = {\"AvgTemp\": \"Average Temperature (°C)\"},\n                  facet_col = \"Region\",\n                  facet_col_spacing = 0.15\n                 )\n\n    fig.update_layout(showlegend=False)\n    return fig"
  },
  {
    "objectID": "posts/Climate Database Visualizations/index.html#line-plot-output",
    "href": "posts/Climate Database Visualizations/index.html#line-plot-output",
    "title": "Climate Database Visualizations",
    "section": "Line Plot Output",
    "text": "Line Plot Output\nBelow, we have the ‘plot_avg_temp_over_time’ function, which creates our visualization for average temperature over time in a country in a given year interval.\n\nfig = plot_avg_temp_over_time(\"temps.db\", \"India\", 1980, 2020, 1)\n\nfig.show()"
  },
  {
    "objectID": "posts/Climate Database Visualizations/index.html#second-visualization",
    "href": "posts/Climate Database Visualizations/index.html#second-visualization",
    "title": "Climate Database Visualizations",
    "section": "Second Visualization",
    "text": "Second Visualization\nThe other way I thought of visualizing temperature change yearly in a given country is by which weather stations recorded the most extreme temperatures. This can be a bar chart showing the stations with the highest and lowest recording temperatures. I believe this is interesting because it can help us identify the most extreme weather situations, which can be useful for things like climate impact studies."
  },
  {
    "objectID": "posts/Climate Database Visualizations/index.html#importing-functions-3",
    "href": "posts/Climate Database Visualizations/index.html#importing-functions-3",
    "title": "Climate Database Visualizations",
    "section": "Importing Functions",
    "text": "Importing Functions\nFirst, I created ‘query_extreme_temps’ which creates a dataframe containing the weather stations with the highest and lowest recorded temperatures using SQL. The dataframe only returns with columns name, MinTemp, and MaxTemp.\n\nfrom climate_database import query_extreme_temps\nprint(inspect.getsource(query_extreme_temps))\n\ndef query_extreme_temps(db_file, country, year_begin, year_end):\n    \"\"\"\n    Gets the weather stations with the highest and lowest recorded temperatures.\n    \"\"\"\n    cmd = f\"\"\"\n        SELECT S.NAME, MIN(T.Temp) AS MinTemp, MAX(T.Temp) AS MaxTemp\n        FROM temperatures T\n        JOIN stations S ON T.ID = S.ID\n        JOIN countries C ON substr(S.ID, 1, 2) = C.\"FIPS 10-4\"\n        WHERE C.Name = '{country}'\n        AND T.Year BETWEEN {year_begin} AND {year_end}\n        GROUP BY S.NAME\n        ORDER BY MaxTemp DESC;\n    \"\"\"\n    with sqlite3.connect(f\"{db_file}\") as conn:\n        df = pd.read_sql_query(cmd, conn)\n        \n    return df\n\n\n\nNext, I created ‘plot_extreme_temps’ which creates a boxplot showing the stations with the highest and lowest temperatures. I define a dataframe using ‘query_extreme_temps’, then using the top_n stations parameter find that many largest and smallest maximum and minimum temperatures from the dataframe. Then I use .melt() to combine these top_n maximum and minimum temperatures and reshape the data. Last is to use plotly’s bar function to create the bar chart with our melted dataframe.\n\nfrom climate_database import plot_extreme_temps\nprint(inspect.getsource(query_avg_temp_over_time))\n\ndef query_avg_temp_over_time(db_file, country, year_begin, year_end, month):\n    \"\"\"\n    Returns the average temperature for each year in a given country as a dataframe.\n    \"\"\"\n    cmd = f\"\"\"\n        SELECT T.Year, S.LATITUDE, AVG(T.Temp) AS AvgTemp\n        FROM temperatures T\n        JOIN stations S ON T.ID = S.ID\n        JOIN countries C ON substr(S.ID, 1, 2) = C.\"FIPS 10-4\"\n        WHERE C.Name = '{country}'\n        AND T.Year BETWEEN {year_begin} AND {year_end}\n        AND T.Month = {month}\n        GROUP BY T.Year, S.LATITUDE\n        ORDER BY T.Year ASC;\n    \"\"\"\n    with sqlite3.connect(f\"{db_file}\") as conn:\n        df = pd.read_sql_query(cmd, conn)\n        \n    return df"
  },
  {
    "objectID": "posts/Climate Database Visualizations/index.html#bar-chat-output",
    "href": "posts/Climate Database Visualizations/index.html#bar-chat-output",
    "title": "Climate Database Visualizations",
    "section": "Bar Chat Output",
    "text": "Bar Chat Output\nBelow, we have the ‘plot_extreme_temps’ function, which creates our visualization for extreme temperatures for a given country and year interval.\n\nfig = plot_extreme_temps(\"temps.db\", \"India\", 1980, 2020, 10)\n\nfig.show()"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Ava Shafran’s Python Portfolio",
    "section": "",
    "text": "Spotify Collaborative Playlist Generator\n\n\n\n\n\n\n\n\n\n\n\nMar 21, 2025\n\n\nAva Shafran, Bea Nelson-Talbot, Annie Cheng\n\n\n\n\n\n\n\n\n\n\n\n\nFake News Detector\n\n\n\n\n\n\nHW\n\n\nVisualizations\n\n\nNN\n\n\nTensorFlow\n\n\n\n\n\n\n\n\n\nMar 16, 2025\n\n\nAva Shafran\n\n\n\n\n\n\n\n\n\n\n\n\n2D Heat Diffusion Method Comparision\n\n\n\n\n\n\nHW\n\n\nJAX\n\n\nNumPy\n\n\nVisualizations\n\n\n\n\n\n\n\n\n\nFeb 26, 2025\n\n\nAva Shafran\n\n\n\n\n\n\n\n\n\n\n\n\nSimple Message Bank with Dash\n\n\n\n\n\n\nHW\n\n\nWeb App\n\n\nDash\n\n\nSQL\n\n\n\n\n\n\n\n\n\nFeb 17, 2025\n\n\nAva Shafran\n\n\n\n\n\n\n\n\n\n\n\n\nTMDB Web Scraping\n\n\n\n\n\n\nHW\n\n\nScrapy\n\n\nWeb Scraping\n\n\n\n\n\n\n\n\n\nFeb 11, 2025\n\n\nAva Shafran\n\n\n\n\n\n\n\n\n\n\n\n\nClimate Database Visualizations\n\n\n\n\n\n\nHW\n\n\nVisualizations\n\n\nSQL\n\n\nPloty\n\n\n\n\n\n\n\n\n\nFeb 2, 2025\n\n\nAva Shafran\n\n\n\n\n\n\n\n\n\n\n\n\nPalmer Penguins Visualization Post\n\n\n\n\n\n\nHW\n\n\nVisualizations\n\n\n\n\n\n\n\n\n\nJan 21, 2025\n\n\nAva Shafran\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\nUCLA Python Course\n\n\n\n\n\n\n\n\n\nJan 21, 2025\n\n\nAva Shafran\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This blog contains a collections of my projects throughout my python course at UCLA. During this time, I honed several skills including working with SQL, Web Apps, Web Scraping, and several visualization tools."
  },
  {
    "objectID": "posts/2D Heat Diffusion Method Comparison/index.html",
    "href": "posts/2D Heat Diffusion Method Comparison/index.html",
    "title": "2D Heat Diffusion Method Comparision",
    "section": "",
    "text": "In this project, I implemented and compared different numerical methods for solving the heat diffusion equation in two dimensions. The heat equation is a fundamental partial differential equation that describes how heat distributes itself over time in a given region. I used NumPy and JAX to implement the finite difference method and visualized the results using Matplotlib. In this post, I’ll walk through the key functions, the visualization process, and the performance comparison of the methods.\n\n\nThe two-dimensional heat equation is given by: \\[\n\\frac{\\partial \\mathrm f(\\mathrm x,\\mathrm y,\\mathrm t)}{\\partial \\mathrm t}\\ = \\frac{\\partial^{2} \\mathrm f}{\\partial \\mathrm x^{2}} + \\frac{\\partial^{2} \\mathrm f}{\\partial \\mathrm y^{2}}\n\\]\nUsing a finite difference discretization, we approximate the equation as: \\[\nu_{i,j}^{k+1} = u_{i,j}^{k} + \\epsilon(u_{i+1,j}^{k} + u_{i-1,j}^{k} + u_{i,j+1}^{k} + u_{i,j-1}^{k} - 4u_{i,j}^{k})\n\\]\nwhere \\(\\epsilon\\) is a stability constant, and \\(u_{i,j}^{k}\\) represents the temperature at grid point \\((i,j)\\) at time step \\(k\\). The boundary conditions are set to zero to allow heat to escape."
  },
  {
    "objectID": "posts/2D Heat Diffusion Method Comparison/index.html#the-heat-equation",
    "href": "posts/2D Heat Diffusion Method Comparison/index.html#the-heat-equation",
    "title": "2D Heat Diffusion Method Comparision",
    "section": "",
    "text": "The two-dimensional heat equation is given by: \\[\n\\frac{\\partial \\mathrm f(\\mathrm x,\\mathrm y,\\mathrm t)}{\\partial \\mathrm t}\\ = \\frac{\\partial^{2} \\mathrm f}{\\partial \\mathrm x^{2}} + \\frac{\\partial^{2} \\mathrm f}{\\partial \\mathrm y^{2}}\n\\]\nUsing a finite difference discretization, we approximate the equation as: \\[\nu_{i,j}^{k+1} = u_{i,j}^{k} + \\epsilon(u_{i+1,j}^{k} + u_{i-1,j}^{k} + u_{i,j+1}^{k} + u_{i,j-1}^{k} - 4u_{i,j}^{k})\n\\]\nwhere \\(\\epsilon\\) is a stability constant, and \\(u_{i,j}^{k}\\) represents the temperature at grid point \\((i,j)\\) at time step \\(k\\). The boundary conditions are set to zero to allow heat to escape."
  },
  {
    "objectID": "posts/2D Heat Diffusion Method Comparison/index.html#initial-condition",
    "href": "posts/2D Heat Diffusion Method Comparison/index.html#initial-condition",
    "title": "2D Heat Diffusion Method Comparision",
    "section": "Initial Condition",
    "text": "Initial Condition\nThe simulation starts with a single heat source at the center of the grid:\n\nN = 101\nepsilon = 0.2\nimport numpy as np\nimport time\nfrom matplotlib import pyplot as plt\nimport numpy as np\nimport jax\nimport jax.numpy as jnp\nfrom jax.experimental import sparse\n\n# Construct initial condition: 1 unit of heat at midpoint.\nu0 = np.zeros((N, N))\nu0[int(N/2), int(N/2)] = 1.0\nplt.imshow(u0)\nplt.show()"
  },
  {
    "objectID": "posts/2D Heat Diffusion Method Comparison/index.html#get_a-constructing-the-finite-difference-matrix",
    "href": "posts/2D Heat Diffusion Method Comparison/index.html#get_a-constructing-the-finite-difference-matrix",
    "title": "2D Heat Diffusion Method Comparision",
    "section": "get_A(): Constructing the Finite Difference Matrix",
    "text": "get_A(): Constructing the Finite Difference Matrix\nThe matrix \\(A\\) represents the finite difference operator for the heat equation. It is a sparse matrix of size \\(N^{2}\\times N^{2}\\), where \\(N\\) is the grid size. The matrix \\(A\\) is constructed using diagonals corresponding to the finite difference stencil.\n\ndef get_A(N):\n    \"\"\" Takes N and returns the corresponding matrix A\n    Args:\n        N: scalar value\n\n    Returns: \n        A: The 2d finite difference matric, N^2 x N^2\n    \"\"\"\n    n = N * N\n    diagonals = [-4 * np.ones(n), np.ones(n-1), np.ones(n-1), np.ones(n-N), np.ones(n-N)]\n    diagonals[1][(N-1)::N] = 0\n    diagonals[2][(N-1)::N] = 0\n    A = np.diag(diagonals[0]) + np.diag(diagonals[1], 1) + np.diag(diagonals[2], -1) + np.diag(diagonals[3], N) + np.diag(diagonals[4], -N)\n    return A"
  },
  {
    "objectID": "posts/2D Heat Diffusion Method Comparison/index.html#advance_time_matvecmul-advancing-the-solution",
    "href": "posts/2D Heat Diffusion Method Comparison/index.html#advance_time_matvecmul-advancing-the-solution",
    "title": "2D Heat Diffusion Method Comparision",
    "section": "advance_time_matvecmul(): Advancing the Solution",
    "text": "advance_time_matvecmul(): Advancing the Solution\nThe advance_time_matvecmul() function advances the solution by one timestep using matrix-vector multiplication. The grid \\(u\\) is flattened into a vector, multiplied by the matrix \\(A\\), and then reshaped back into a grid.\n\ndef advance_time_matvecmul(A, u, epsilon):\n    \"\"\" Advances the simulation by one timestep, via matrix-vector multiplication\n    Args:\n        A: The 2d finite difference matrix, N^2 x N^2. \n        u: N x N grid state at timestep k.\n        epsilon: stability constant.\n\n    Returns:\n        N x N Grid state at timestep k+1.\n    \"\"\"\n    N = u.shape[0]\n    u = u + epsilon * (A @ u.flatten()).reshape((N, N))\n    return u"
  },
  {
    "objectID": "posts/2D Heat Diffusion Method Comparison/index.html#running-the-simulation",
    "href": "posts/2D Heat Diffusion Method Comparison/index.html#running-the-simulation",
    "title": "2D Heat Diffusion Method Comparision",
    "section": "Running the Simulation",
    "text": "Running the Simulation\nThe simulation is run for 2700 iterations, and snapshots are taken every 300 iterations for visualization.\n\nA = get_A(N)\nsnapshots0 = []\n\nstart_time = time.time()\nfor i in range(2701):\n    u0 = advance_time_matvecmul(A, u0, epsilon)\n    if i &gt; 0 and i % 300 == 0:\n        snapshots0.append(u0.copy())  # Store intermediate results for visualization\nend_time = time.time()\n\nprint(f\"Simulation time: {end_time - start_time:.2f} seconds\")\n\n# Visualization\nX, Y = np.meshgrid(range(N), range(N))\nfig0, axes0 = plt.subplots(3, 3, figsize=(12, 10))\n\nfor i, ax in enumerate(axes0.flatten()):\n    if i &lt; len(snapshots0):\n        contourf = ax.contourf(X, Y, snapshots0[i], levels=20, cmap='viridis')\n        ax.set_title(f'Iteration {(i+1) * 300}')\n\nfig0.tight_layout(rect=[0, 0, 0.9, 1])\ncbar_ax = fig0.add_axes([0.92, 0.1, 0.03, 0.8])\nfig0.colorbar(contourf, cax=cbar_ax)\n\nplt.show()\n\nSimulation time: 86.88 seconds\n\n\n\n\n\n\n\n\n\nThis method is straightforward but computationally expensive due to the large size of \\(A\\). Running the simulation for 2700 iterations took about 49.14 seconds."
  },
  {
    "objectID": "posts/2D Heat Diffusion Method Comparison/index.html#get_sparse_a-constructing-the-sparse-matrix",
    "href": "posts/2D Heat Diffusion Method Comparison/index.html#get_sparse_a-constructing-the-sparse-matrix",
    "title": "2D Heat Diffusion Method Comparision",
    "section": "get_sparse_A(): Constructing the Sparse Matrix",
    "text": "get_sparse_A(): Constructing the Sparse Matrix\nTo improve performance, we use a sparse matrix representation of \\(A\\). The get_sparse_A() function constructs the matrix \\(A\\) in JAX’s sparse BCOO format.\n\ndef get_sparse_A(N):\n    \"\"\" A function which returns matrix A in a sparse format given N\n    Args:\n        N: scalar value\n\n    Returns:\n        A_sp_matrix: matrix A in a sparse format\n    \"\"\"\n    # Use previously defined function to define A\n    A = get_A(N)\n    \n    # Convert to JAX sparse BCOO format\n    A_sp_matrix = sparse.BCOO.fromdense(jnp.array(A))\n\n    return A_sp_matrix"
  },
  {
    "objectID": "posts/2D Heat Diffusion Method Comparison/index.html#running-the-simulation-1",
    "href": "posts/2D Heat Diffusion Method Comparison/index.html#running-the-simulation-1",
    "title": "2D Heat Diffusion Method Comparision",
    "section": "Running the Simulation",
    "text": "Running the Simulation\nThe simulation is run for 2700 iterations, and snapshots are taken every 300 iterations for visualization.\n\nu1 = jnp.zeros((N, N))\nu1 = u1.at[int(N/2), int(N/2)].set(1.0)  # JAX-compatible way to set values\n\nA = get_sparse_A(N)\nsnapshots1 = []\n\nstart_time = time.time()\nfor i in range(2701):\n    u1 = advance_time_matvecmul(A, u1, epsilon)\n    if i &gt; 0 and i % 300 == 0:\n        snapshots1.append(u1.copy())  # Store intermediate results for visualization\nend_time = time.time()\n\nprint(f\"Simulation time: {end_time - start_time:.2f} seconds\")\n\n# Visualization\nX, Y = np.meshgrid(range(N), range(N))\nfig1, axes1 = plt.subplots(3, 3, figsize=(12, 10))\n\nfor i, ax in enumerate(axes1.flatten()):\n    if i &lt; len(snapshots1):\n        contourf = ax.contourf(X, Y, snapshots1[i], levels=20, cmap='viridis')\n        ax.set_title(f'Iteration {(i+1) * 300}')\n\nfig1.tight_layout(rect=[0, 0, 0.9, 1])\ncbar_ax = fig1.add_axes([0.92, 0.1, 0.03, 0.8])\nfig1.colorbar(contourf, cax=cbar_ax)\n\nplt.show()\n\nSimulation time: 12.09 seconds\n\n\n\n\n\n\n\n\n\nUsing the sparse matrix significantly reduces memory usage and computation time. Running the simulation for 2700 iterations took about 6.38 seconds. The results are identical to the dense matrix approach but computed much faster."
  },
  {
    "objectID": "posts/2D Heat Diffusion Method Comparison/index.html#advance_time_numpy-advancing-the-solution",
    "href": "posts/2D Heat Diffusion Method Comparison/index.html#advance_time_numpy-advancing-the-solution",
    "title": "2D Heat Diffusion Method Comparision",
    "section": "advance_time_numpy(): Advancing the Solution",
    "text": "advance_time_numpy(): Advancing the Solution\nThe advance_time_numpy() function advances the solution using direct array operations with np.roll(). This avoids the need for matrix-vector multiplication and is more efficient.\n\ndef advance_time_numpy(u, epsilon):\n    \"\"\" Advances solution by one timestep by using np.roll\n     Args:\n        u: N x N grid state at timestep k (NumPy array).\n        epsilon: Stability constant.\n\n    Returns:\n        Updated N x N grid state at timestep k+1 (NumPy array).\n    \"\"\"\n    # Pad the grid with zeros to handle boundary conditions\n    u_padded = np.pad(u, pad_width=1, mode='constant', constant_values=0)\n\n    # Compute the Laplacian using finite differences\n    laplacian = (\n        u_padded[2:, 1:-1] +  # u_{i+1,j}\n        u_padded[:-2, 1:-1] + # u_{i-1,j}\n        u_padded[1:-1, 2:] +  # u_{i,j+1}\n        u_padded[1:-1, :-2] - # u_{i,j-1}\n        4 * u\n    )\n\n    # Update the heat distribution\n    u_new = u + epsilon * laplacian\n\n    return u_new"
  },
  {
    "objectID": "posts/2D Heat Diffusion Method Comparison/index.html#running-the-simulation-2",
    "href": "posts/2D Heat Diffusion Method Comparison/index.html#running-the-simulation-2",
    "title": "2D Heat Diffusion Method Comparision",
    "section": "Running the Simulation",
    "text": "Running the Simulation\nThe simulation is run for 2700 iterations, and snapshots are taken every 300 iterations for visualization.\n\nu2 = np.zeros((N, N))\nu2[N//2, N//2] = 1.0  # Set heat source at the center\n\nsnapshots2 = []\n\nstart_time = time.time()\nfor i in range(2701):\n    u2 = advance_time_numpy(u2, epsilon)\n\n    if i &gt; 0 and i % 300 == 0:  # Store every 300 iterations\n        snapshots2.append(u2.copy())\nend_time = time.time()\n\nprint(f\"NumPy Direct Execution Time: {end_time - start_time:.2f} seconds\")\n\n# Visualization\nX, Y = np.meshgrid(range(N), range(N))\nfig2, axes2 = plt.subplots(3, 3, figsize=(12, 10))\n\nfor i, ax in enumerate(axes2.flatten()):\n    if i &lt; len(snapshots2):\n        contourf = ax.contourf(X, Y, snapshots2[i], levels=20, cmap='viridis')\n        ax.set_title(f'Iteration {(i+1) * 300}')\n\nfig2.tight_layout(rect=[0, 0, 0.9, 1])\ncbar_ax = fig2.add_axes([0.92, 0.1, 0.03, 0.8])\nfig2.colorbar(contourf, cax=cbar_ax)\n\nplt.show()\n\nNumPy Direct Execution Time: 0.38 seconds\n\n\n\n\n\n\n\n\n\nThis method is faster than the matrix-vector multiplication approach. Running the simulation for 2700 iterations took about 0.18 seconds. The results are consistent with the previous methods."
  },
  {
    "objectID": "posts/2D Heat Diffusion Method Comparison/index.html#advance_time_jax-advancing-the-solution",
    "href": "posts/2D Heat Diffusion Method Comparison/index.html#advance_time_jax-advancing-the-solution",
    "title": "2D Heat Diffusion Method Comparision",
    "section": "advance_time_jax(): Advancing the Solution",
    "text": "advance_time_jax(): Advancing the Solution\nThe advance_time_jax() function uses JAX’s just-in-time (JIT) compilation to accelerate the computation. It follows the same logic as the NumPy implementation but leverages JAX’s performance optimizations.\n\n@jax.jit\ndef advance_time_jax(u, epsilon):\n    \"\"\"Using just-in-time (jit) compilitation to advance solution by one timestep\n    Args: \n        u: N x N grid state at time k \n        epsilon: Stability constant\n\n    Returns:\n        u_new: Updated N x N grid state at timestep k+1\n    \"\"\"\n    # Pad the grid with zeros to handle boundary conditions\n    u_padded = jnp.pad(u, pad_width=1, mode='constant', constant_values=0)\n\n    # Compute the Laplacian using finite differences\n    laplacian = (\n        u_padded[2:, 1:-1] +  # u_{i+1,j}\n        u_padded[:-2, 1:-1] + # u_{i-1,j}\n        u_padded[1:-1, 2:] +  # u_{i,j+1}\n        u_padded[1:-1, :-2] - # u_{i,j-1}\n        4 * u\n    )\n\n    # Update the heat distribution\n    u_new = u + epsilon * laplacian\n\n    return u_new"
  },
  {
    "objectID": "posts/2D Heat Diffusion Method Comparison/index.html#running-the-simulation-3",
    "href": "posts/2D Heat Diffusion Method Comparison/index.html#running-the-simulation-3",
    "title": "2D Heat Diffusion Method Comparision",
    "section": "Running the Simulation",
    "text": "Running the Simulation\nThe simulation is run for 2700 iterations, and snapshots are taken every 300 iterations for visualization.\n\nu3 = jnp.zeros((N, N))\nu3 = u3.at[N//2, N//2].set(1.0)  # JAX-compatible way to modify array\n\n# JAX Warm-up (first run triggers compilation)\nu3 = advance_time_jax(u3, epsilon)\n\nsnapshots3 = []\n\nstart_time = time.time()\nfor i in range(2701):\n    u3 = advance_time_jax(u3, epsilon)\n    \n    if i &gt; 0 and i % 300 == 0:  # Store every 300 iterations\n        snapshots3.append(np.array(u3))  # Convert to NumPy for plotting\n\nend_time = time.time()\nprint(f\"JAX JIT Execution Time: {end_time - start_time:.2f} seconds\")\n\n# Visualization\nX, Y = np.meshgrid(range(N), range(N))\nfig3, axes3 = plt.subplots(3, 3, figsize=(12, 10))\n\nfor i, ax in enumerate(axes3.flatten()):\n    if i &lt; len(snapshots3):\n        contourf = ax.contourf(X, Y, snapshots3[i], levels=20, cmap='viridis')\n        ax.set_title(f'Iteration {(i+1) * 300}')\n\nfig3.tight_layout(rect=[0, 0, 0.9, 1])\ncbar_ax = fig3.add_axes([0.92, 0.1, 0.03, 0.8])\nfig3.colorbar(contourf, cax=cbar_ax)\n\nplt.show()\n\nJAX JIT Execution Time: 0.05 seconds\n\n\n\n\n\n\n\n\n\nThis method is the fastest, thanks to JAX’s JIT compilation. Running the simulation for 2700 iterations took about 0.03 seconds. The results are consistent with the other methods."
  },
  {
    "objectID": "posts/2D Heat Diffusion Method Comparison/index.html#comparison-of-methods",
    "href": "posts/2D Heat Diffusion Method Comparison/index.html#comparison-of-methods",
    "title": "2D Heat Diffusion Method Comparision",
    "section": "Comparison of Methods",
    "text": "Comparison of Methods\n\n\n\n\n\n\n\n\nMethod\nExecution Time\nEase of Implementation\n\n\n\n\nMatrix-Vector Multiplication\n49.14 seconds\nModerate\n\n\nSparse Matrix (JAX)\n6.38 seconds\nModerate\n\n\nDirect Operation (NumPy)\n0.18 seconds\nEasy\n\n\nJAX with JIT\n0.03 seconds\nEasy\n\n\n\nOverall, the fastest method is JAX with JIT compilation, while the easiest to implement is the Direct operation with NumPy."
  },
  {
    "objectID": "posts/Fake News Detector/index.html",
    "href": "posts/Fake News Detector/index.html",
    "title": "Fake News Detector",
    "section": "",
    "text": "With the rise of social media and online news platforms, the volume of information being shared has skyrocketed, creating an urgent need for automated systems that can reliably distinguish between credible news and fabricated stories.\nIn this blog post, we’ll explore how machine learning can be used to tackle this problem. Using a dataset of news articles labeled as real or fake, we’ll build and evaluate three different deep learning models to determine whether it’s more effective to focus on the title, the full text, or both when detecting fake news. By comparing the performance of these models, we’ll provide insights into the best strategies for building robust fake news detection systems.\n\n\nThe dataset we’ll be using comes from the paper “Detection of Online Fake News Using N-Gram Analysis and Machine Learning Techniques” by Ahmed H, Traore I, and Saad S (2017), published in the Lecture Notes in Computer Science series. This dataset was accessed via Kaggle, a popular platform for data science and machine learning resources. It contains news articles labeled as either real (0) or fake (1), with columns for the article title, full text, and the corresponding label. The dataset has already been preprocessed and split into training and testing sets, allowing us to focus on model development and evaluation."
  },
  {
    "objectID": "posts/Fake News Detector/index.html#dataset",
    "href": "posts/Fake News Detector/index.html#dataset",
    "title": "Fake News Detector",
    "section": "",
    "text": "The dataset we’ll be using comes from the paper “Detection of Online Fake News Using N-Gram Analysis and Machine Learning Techniques” by Ahmed H, Traore I, and Saad S (2017), published in the Lecture Notes in Computer Science series. This dataset was accessed via Kaggle, a popular platform for data science and machine learning resources. It contains news articles labeled as either real (0) or fake (1), with columns for the article title, full text, and the corresponding label. The dataset has already been preprocessed and split into training and testing sets, allowing us to focus on model development and evaluation."
  },
  {
    "objectID": "posts/Fake News Detector/index.html#validation-data",
    "href": "posts/Fake News Detector/index.html#validation-data",
    "title": "Fake News Detector",
    "section": "Validation Data",
    "text": "Validation Data\nWe can now split the dataset into: - 80% Training Set - 20% Validation Set\nWe use .take() and .skip() to extract the subsets.\n\ntrain_size = int(0.8*len(Dataset))\nval_size = int(0.2*len(Dataset))\n\ntrain = Dataset.take(train_size)\nval = Dataset.skip(train_size).take(val_size)\n\nlen(train), len(val)\n\n(180, 45)"
  },
  {
    "objectID": "posts/Fake News Detector/index.html#base-rate",
    "href": "posts/Fake News Detector/index.html#base-rate",
    "title": "Fake News Detector",
    "section": "Base Rate",
    "text": "Base Rate\nThe base rate is the accuracy of a model that always makes the same guess. We can determine the base rate for this data set by: - Extracting the fake column from the training dataset. - Counting how many articles are real (0) and fake (1). - Computing the base rate.\n\n# Get an iterator for the 'fake' values on the training dataset\nfake_iterator = train.map(lambda article,fake: fake).as_numpy_iterator()\n\n# Convert to a list of values\nfake_list = list(fake_iterator)\nfake_list = [item for sublist in fake_list for item in sublist]\n\n# Count occurences of each label\nnum_true = sum(1 for label in fake_list if label == 0)\nnum_fake = sum(1 for label in fake_list if label == 1)\n\nprint(f\"Number of true articles: {num_true}\")\nprint(f\"Number of fake articles: {num_fake}\")\n\n# Calculate base rate\ntotal = num_true + num_fake\nmost_frequent = max(num_true, num_fake)\nbase_rate = most_frequent / total\n\nprint(f\"Base rate: {base_rate}\")\n\nNumber of true articles: 8603\nNumber of fake articles: 9397\nBase rate: 0.5220555555555556\n\n\nThe base rate of about 0.5221 tells us that our base model would predict ‘fake news’ correctly about 52.21% of the time."
  },
  {
    "objectID": "posts/Fake News Detector/index.html#text-vectorization",
    "href": "posts/Fake News Detector/index.html#text-vectorization",
    "title": "Fake News Detector",
    "section": "Text Vectorization",
    "text": "Text Vectorization\nThe purpose of text vectorization is to covert text into a numerical format for input into a neural network. - standardization() is used to convert text to lowercase and remove punctuation. - The TextVectorization layer limits vocab to 2000 unique words, converts words to integers, and ensures each sequence has a fixed length of 500. - .adapt() is used to learn vocabulary from the training titles. - The Embedding layer converts word indices into dense vectors of fixed size (embedding_dim = 256).\n\n#preparing a text vectorization layer for tf model\nsize_vocabulary = 2000\n\ndef standardization(input_data):\n    lowercase = tf.strings.lower(input_data)\n    no_punctuation = tf.strings.regex_replace(lowercase,\n                                  '[%s]' % re.escape(string.punctuation),'')\n    return no_punctuation\n\nembedding_dim = 256  # Embedding vector size\n\n# Shared text vectorization layer (for both title & text)\ntext_vectorize_layer = TextVectorization(\n    standardize=standardization,\n    max_tokens=size_vocabulary,\n    output_mode='int',\n    output_sequence_length=500  # Ensures consistent input size\n)\n\n# Adapt on both titles and text\ntext_vectorize_layer.adapt(train.map(lambda x, y: tf.concat([x[\"title\"], x[\"text\"]], axis=0)))\n\n# Shared embedding layer\nembedding_layer = layers.Embedding(\n    input_dim=size_vocabulary,\n    output_dim=embedding_dim,\n    mask_zero=True  # Helps LSTM ignore padding tokens\n)"
  },
  {
    "objectID": "posts/Fake News Detector/index.html#model-visualization-function",
    "href": "posts/Fake News Detector/index.html#model-visualization-function",
    "title": "Fake News Detector",
    "section": "Model Visualization Function",
    "text": "Model Visualization Function\nPart of our analysis of models will be visualization the training and validation accuracy to compare performance. vizualize_model() will plot both the training accuracy and validation accuracy, which we can use on each of our three models to help us determine if one model is better than another.\n\ndef plot_training(history, title):\n    plt.plot(history.history[\"accuracy\"], label=\"Train Accuracy\")\n    plt.plot(history.history[\"val_accuracy\"], label=\"Val Accuracy\")\n    plt.title(title)\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Accuracy\")\n    plt.legend()\n    plt.show()"
  },
  {
    "objectID": "posts/Fake News Detector/index.html#first-model",
    "href": "posts/Fake News Detector/index.html#first-model",
    "title": "Fake News Detector",
    "section": "First Model",
    "text": "First Model\nThis first model will only use the article title as an input. The model then vectorizes and embeds the title. We use an LSTM layer to extract sequential features. This outputs a single neuron with a sigmoid activation for binary classification.\n\ndef create_title_model():\n    title_input = layers.Input(shape=(1,), dtype=tf.string, name=\"title\")\n\n    # Process title\n    title_vectorized = text_vectorize_layer(title_input)\n    title_embedded = embedding_layer(title_vectorized)\n    title_lstm = layers.LSTM(64)(title_embedded)\n\n    # Output layer\n    output = layers.Dense(1, activation=\"sigmoid\")(title_lstm)\n\n    # Build model\n    model = keras.Model(inputs=title_input, outputs=output, name=\"title_model\")\n\n    # Compile model\n    model.compile(loss=\"binary_crossentropy\", optimizer=keras.optimizers.Adam(learning_rate=0.0001), metrics=[\"accuracy\"])\n\n    return model\n\ntitle_model = create_title_model()\ntitle_model.summary()\n\nModel: \"title_model\"\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ Layer (type)              ┃ Output Shape           ┃        Param # ┃ Connected to           ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ title (InputLayer)        │ (None, 1)              │              0 │ -                      │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ text_vectorization_1      │ (None, 500)            │              0 │ title[0][0]            │\n│ (TextVectorization)       │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ embedding_1 (Embedding)   │ (None, 500, 256)       │        512,000 │ text_vectorization_1[… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ not_equal_6 (NotEqual)    │ (None, 500)            │              0 │ text_vectorization_1[… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ lstm_6 (LSTM)             │ (None, 64)             │         82,176 │ embedding_1[0][0],     │\n│                           │                        │                │ not_equal_6[0][0]      │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense_6 (Dense)           │ (None, 1)              │             65 │ lstm_6[0][0]           │\n└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n\n\n\n Total params: 594,241 (2.27 MB)\n\n\n\n Trainable params: 594,241 (2.27 MB)\n\n\n\n Non-trainable params: 0 (0.00 B)\n\n\n\nWe can visualize our model using plot_model():\n\nutils.plot_model(title_model, \"title_model.png\",\n                        show_shapes=True,\n                        show_layer_names=True)\n\n\n\n\n\n\n\n\ntitle_model begins with an input layer for the title text, which is passed through a shared TextVectorization layer to convert the text into integer sequences. These sequences are then fed into a shared Embedding layer, which maps the words into dense vectors of fixed size. The embedded title is processed by an LSTM layer to capture sequential dependencies in the text. Finally, a dense output layer with a sigmoid activation produces the binary classification (real or fake news). This model is lightweight and efficient but relies solely on the limited information provided by article titles."
  },
  {
    "objectID": "posts/Fake News Detector/index.html#first-model-training-and-visualization",
    "href": "posts/Fake News Detector/index.html#first-model-training-and-visualization",
    "title": "Fake News Detector",
    "section": "First Model Training and Visualization",
    "text": "First Model Training and Visualization\nWe can increase the number of Epochs used when we implement early stopping. Early stopping will stop out model when its performance starts to decrease, and prevents overfitting.\n\nearly_stopping = keras.callbacks.EarlyStopping(\n    monitor=\"val_accuracy\",\n    patience=3,\n    restore_best_weights=True\n)\n\n# Train title model\ntitle_history = title_model.fit(\n    train.map(lambda x, y: (x[\"title\"], y)),\n    validation_data=val.map(lambda x, y: (x[\"title\"], y)),\n    epochs=20,\n    callbacks=[early_stopping]\n)\n\nEpoch 1/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 40s 15ms/step - accuracy: 0.5936 - loss: 0.6440 - val_accuracy: 0.8863 - val_loss: 0.3368\nEpoch 2/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 12ms/step - accuracy: 0.9010 - loss: 0.2849 - val_accuracy: 0.9121 - val_loss: 0.2285\nEpoch 3/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 11ms/step - accuracy: 0.9278 - loss: 0.1949 - val_accuracy: 0.9229 - val_loss: 0.1981\nEpoch 4/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 13ms/step - accuracy: 0.9397 - loss: 0.1600 - val_accuracy: 0.9283 - val_loss: 0.1842\nEpoch 5/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 13ms/step - accuracy: 0.9480 - loss: 0.1399 - val_accuracy: 0.9317 - val_loss: 0.1779\nEpoch 6/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 11ms/step - accuracy: 0.9526 - loss: 0.1268 - val_accuracy: 0.9319 - val_loss: 0.1757\nEpoch 7/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 11ms/step - accuracy: 0.9562 - loss: 0.1175 - val_accuracy: 0.9308 - val_loss: 0.1758\nEpoch 8/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 11ms/step - accuracy: 0.9591 - loss: 0.1105 - val_accuracy: 0.9319 - val_loss: 0.1773\nEpoch 9/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 14ms/step - accuracy: 0.9611 - loss: 0.1049 - val_accuracy: 0.9314 - val_loss: 0.1798\n\n\n\nplot_training(title_history, \"Title Model Accuracy\")"
  },
  {
    "objectID": "posts/Fake News Detector/index.html#third-model-performance-analysis",
    "href": "posts/Fake News Detector/index.html#third-model-performance-analysis",
    "title": "Fake News Detector",
    "section": "Third Model Performance Analysis",
    "text": "Third Model Performance Analysis\nOur title model ended with a validation accuracy of about 93%. This is still a high performance, but we would like to get a score of at least 97%. This suggests the titles alone may not be enough information to reliably distinguish between real and fake news."
  },
  {
    "objectID": "posts/Fake News Detector/index.html#second-model",
    "href": "posts/Fake News Detector/index.html#second-model",
    "title": "Fake News Detector",
    "section": "Second Model",
    "text": "Second Model\nIn the second model, we only use the article text as an input. This model is similar to model 1, but uses an LSTM layer to capture long-range dependencies in an article text.\n\ndef create_text_model():\n    text_input = layers.Input(shape=(1,), dtype=tf.string, name=\"text\")\n\n    # Process text\n    text_vectorized = text_vectorize_layer(text_input)\n    text_embedded = embedding_layer(text_vectorized)\n    text_lstm = layers.LSTM(64, use_cudnn=False)(text_embedded)\n\n    # Output layer\n    output = layers.Dense(1, activation=\"sigmoid\")(text_lstm)\n\n    # Build model\n    model = keras.Model(inputs=text_input, outputs=output, name=\"text_model\")\n\n    # Compile model\n    model.compile(loss=\"binary_crossentropy\", optimizer=keras.optimizers.Adam(learning_rate=0.0001), metrics=[\"accuracy\"])\n\n    return model\n\ntext_model = create_text_model()\ntext_model.summary()\n\nModel: \"text_model\"\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ Layer (type)              ┃ Output Shape           ┃        Param # ┃ Connected to           ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ text (InputLayer)         │ (None, 1)              │              0 │ -                      │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ text_vectorization_1      │ (None, 500)            │              0 │ text[0][0]             │\n│ (TextVectorization)       │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ embedding_1 (Embedding)   │ (None, 500, 256)       │        512,000 │ text_vectorization_1[… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ not_equal_7 (NotEqual)    │ (None, 500)            │              0 │ text_vectorization_1[… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ lstm_7 (LSTM)             │ (None, 64)             │         82,176 │ embedding_1[1][0],     │\n│                           │                        │                │ not_equal_7[0][0]      │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense_7 (Dense)           │ (None, 1)              │             65 │ lstm_7[0][0]           │\n└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n\n\n\n Total params: 594,241 (2.27 MB)\n\n\n\n Trainable params: 594,241 (2.27 MB)\n\n\n\n Non-trainable params: 0 (0.00 B)\n\n\n\n\nutils.plot_model(text_model, \"text_model.png\",\n                        show_shapes=True,\n                        show_layer_names=True)\n\n\n\n\n\n\n\n\nLike the title_model, text_model uses shared TextVectorization and Embedding layers to preprocess the input. However, the text input is significantly longer, so the LSTM layer has to handle much larger sequences. This model captures more detailed contextual information from the article text, which explains its higher validation accuracy compared to the title_model. However, the increased complexity also results in longer training times and higher computational costs."
  },
  {
    "objectID": "posts/Fake News Detector/index.html#second-model-training-and-visualization",
    "href": "posts/Fake News Detector/index.html#second-model-training-and-visualization",
    "title": "Fake News Detector",
    "section": "Second Model training and Visualization",
    "text": "Second Model training and Visualization\n\n# Train text model\ntext_history = text_model.fit(\n    train.map(lambda x, y: (x[\"text\"], y)),\n    validation_data=val.map(lambda x, y: (x[\"text\"], y)),\n    epochs=20,\n    callbacks=[early_stopping]\n)\n\nEpoch 1/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 205s 1s/step - accuracy: 0.6597 - loss: 0.6272 - val_accuracy: 0.9238 - val_loss: 0.2476\nEpoch 2/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 262s 1s/step - accuracy: 0.9217 - loss: 0.2360 - val_accuracy: 0.9022 - val_loss: 0.2543\nEpoch 3/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 262s 1s/step - accuracy: 0.9343 - loss: 0.1940 - val_accuracy: 0.9164 - val_loss: 0.2657\nEpoch 4/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 261s 1s/step - accuracy: 0.9386 - loss: 0.1831 - val_accuracy: 0.9514 - val_loss: 0.1345\nEpoch 5/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 266s 1s/step - accuracy: 0.9503 - loss: 0.1536 - val_accuracy: 0.9488 - val_loss: 0.2114\nEpoch 6/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 273s 1s/step - accuracy: 0.9589 - loss: 0.1544 - val_accuracy: 0.9708 - val_loss: 0.1201\nEpoch 7/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 250s 1s/step - accuracy: 0.9720 - loss: 0.1131 - val_accuracy: 0.9742 - val_loss: 0.1083\nEpoch 8/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 272s 1s/step - accuracy: 0.9752 - loss: 0.1060 - val_accuracy: 0.9768 - val_loss: 0.0979\nEpoch 9/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 263s 1s/step - accuracy: 0.9787 - loss: 0.0936 - val_accuracy: 0.9735 - val_loss: 0.1064\nEpoch 10/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 251s 1s/step - accuracy: 0.9802 - loss: 0.0899 - val_accuracy: 0.9773 - val_loss: 0.0945\nEpoch 11/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 270s 1s/step - accuracy: 0.9802 - loss: 0.0896 - val_accuracy: 0.9755 - val_loss: 0.0951\nEpoch 12/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 262s 1s/step - accuracy: 0.9842 - loss: 0.0764 - val_accuracy: 0.9784 - val_loss: 0.0919\nEpoch 13/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 253s 1s/step - accuracy: 0.9855 - loss: 0.0718 - val_accuracy: 0.9793 - val_loss: 0.0874\nEpoch 14/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 273s 1s/step - accuracy: 0.9875 - loss: 0.0664 - val_accuracy: 0.9784 - val_loss: 0.0884\nEpoch 15/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 262s 1s/step - accuracy: 0.9901 - loss: 0.0606 - val_accuracy: 0.9795 - val_loss: 0.0875\nEpoch 16/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 262s 1s/step - accuracy: 0.9726 - loss: 0.1060 - val_accuracy: 0.9323 - val_loss: 0.2165\nEpoch 17/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 261s 1s/step - accuracy: 0.9551 - loss: 0.1703 - val_accuracy: 0.9672 - val_loss: 0.1105\nEpoch 18/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 250s 1s/step - accuracy: 0.9735 - loss: 0.0987 - val_accuracy: 0.9663 - val_loss: 0.1095\n\n\n\nplot_training(text_history, \"Text Model Accuracy\")\n\n\n\n\n\n\n\n\nFor the text model, the validation accuracy is about 97%, but there was a drop in validation accuracy during training which we can see in the visualization. This model took significantly longer to train due to the larger input size. We can deduce that the full text of an article contains more informative features for detecting fake new."
  },
  {
    "objectID": "posts/Fake News Detector/index.html#model-3",
    "href": "posts/Fake News Detector/index.html#model-3",
    "title": "Fake News Detector",
    "section": "Model 3",
    "text": "Model 3\nFor the third model, we use both the article title and text as inputs. We extract features separately using LSTM layers for both. Then we merge the two feature vectors and process them through a Dense() layer with dropout to prevent overfitting. This outputs a sigmoid neuron for classification.\n\ndef create_combined_model():\n    # Title input branch\n    title_input = layers.Input(shape=(1,), dtype=tf.string, name=\"title\")\n    title_vectorized = text_vectorize_layer(title_input)\n    title_embedded = embedding_layer(title_vectorized)\n    title_lstm = layers.LSTM(64)(title_embedded)\n\n    # Text input branch\n    text_input = layers.Input(shape=(1,), dtype=tf.string, name=\"text\")\n    text_vectorized = text_vectorize_layer(text_input)\n    text_embedded = embedding_layer(text_vectorized)\n    text_lstm = layers.LSTM(64, use_cudnn=False)(text_embedded)\n\n    # Combine both branches\n    merged = layers.concatenate([title_lstm, text_lstm])\n\n    # Fully connected layers\n    dense1 = layers.Dense(64, activation=\"relu\")(merged)\n    dropout = layers.Dropout(0.3)(dense1)\n\n    # Output layer\n    output = layers.Dense(1, activation=\"sigmoid\")(dropout)\n\n    # Build model\n    model = keras.Model(inputs=[title_input, text_input], outputs=output, name=\"combined_model\")\n\n    # Compile model\n    model.compile(loss=\"binary_crossentropy\", optimizer=keras.optimizers.Adam(learning_rate=0.0001), metrics=[\"accuracy\"])\n\n    return model\n\ncombined_model = create_combined_model()\ncombined_model.summary()\n\nModel: \"combined_model\"\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ Layer (type)              ┃ Output Shape           ┃        Param # ┃ Connected to           ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ title (InputLayer)        │ (None, 1)              │              0 │ -                      │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ text (InputLayer)         │ (None, 1)              │              0 │ -                      │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ text_vectorization_1      │ (None, 500)            │              0 │ title[0][0],           │\n│ (TextVectorization)       │                        │                │ text[0][0]             │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ embedding_1 (Embedding)   │ (None, 500, 256)       │        512,000 │ text_vectorization_1[… │\n│                           │                        │                │ text_vectorization_1[… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ not_equal_8 (NotEqual)    │ (None, 500)            │              0 │ text_vectorization_1[… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ not_equal_9 (NotEqual)    │ (None, 500)            │              0 │ text_vectorization_1[… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ lstm_8 (LSTM)             │ (None, 64)             │         82,176 │ embedding_1[2][0],     │\n│                           │                        │                │ not_equal_8[0][0]      │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ lstm_9 (LSTM)             │ (None, 64)             │         82,176 │ embedding_1[3][0],     │\n│                           │                        │                │ not_equal_9[0][0]      │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ concatenate_2             │ (None, 128)            │              0 │ lstm_8[0][0],          │\n│ (Concatenate)             │                        │                │ lstm_9[0][0]           │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense_8 (Dense)           │ (None, 64)             │          8,256 │ concatenate_2[0][0]    │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dropout_2 (Dropout)       │ (None, 64)             │              0 │ dense_8[0][0]          │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense_9 (Dense)           │ (None, 1)              │             65 │ dropout_2[0][0]        │\n└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n\n\n\n Total params: 684,673 (2.61 MB)\n\n\n\n Trainable params: 684,673 (2.61 MB)\n\n\n\n Non-trainable params: 0 (0.00 B)\n\n\n\n\nutils.plot_model(combined_model, \"combined_model.png\",\n                        show_shapes=True,\n                        show_layer_names=True)\n\n\n\n\n\n\n\n\nThe combined_model processes the title and text separately through shared TextVectorization and Embedding layers, ensuring consistency in how words are represented. Each input branch includes an LSTM layer to capture sequential information. The outputs of these branches are concatenated and passed through a dense layer with dropout for regularization. Finally, a sigmoid output layer produces the classification. This architecture allows the model to combine the concise information from titles with the detailed context from the full text, resulting in the highest validation accuracy of the three models. The visualization clearly shows how the two input branches merge, demonstrating the model’s ability to integrate multiple sources of information effectively."
  },
  {
    "objectID": "posts/Fake News Detector/index.html#third-model-training-and-visualization",
    "href": "posts/Fake News Detector/index.html#third-model-training-and-visualization",
    "title": "Fake News Detector",
    "section": "Third Model Training and Visualization",
    "text": "Third Model Training and Visualization\n\n# Train combined model\ncombined_history = combined_model.fit(\n    train.map(lambda x, y: ({\"title\": x[\"title\"], \"text\": x[\"text\"]}, y)),\n    validation_data=val.map(lambda x, y: ({\"title\": x[\"title\"], \"text\": x[\"text\"]}, y)),\n    epochs=20,\n    callbacks=[early_stopping]\n)\n\nEpoch 1/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 208s 1s/step - accuracy: 0.8477 - loss: 0.5661 - val_accuracy: 0.9708 - val_loss: 0.1216\nEpoch 2/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 205s 1s/step - accuracy: 0.9741 - loss: 0.0986 - val_accuracy: 0.9746 - val_loss: 0.0711\nEpoch 3/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 207s 1s/step - accuracy: 0.9814 - loss: 0.0567 - val_accuracy: 0.9836 - val_loss: 0.0552\nEpoch 4/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 268s 1s/step - accuracy: 0.9890 - loss: 0.0364 - val_accuracy: 0.9845 - val_loss: 0.0465\nEpoch 5/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 204s 1s/step - accuracy: 0.9916 - loss: 0.0289 - val_accuracy: 0.9856 - val_loss: 0.0404\nEpoch 6/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 202s 1s/step - accuracy: 0.9905 - loss: 0.0318 - val_accuracy: 0.9845 - val_loss: 0.0460\nEpoch 7/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 263s 1s/step - accuracy: 0.9924 - loss: 0.0251 - val_accuracy: 0.9876 - val_loss: 0.0350\nEpoch 8/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 260s 1s/step - accuracy: 0.9952 - loss: 0.0188 - val_accuracy: 0.9870 - val_loss: 0.0360\nEpoch 9/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 204s 1s/step - accuracy: 0.9957 - loss: 0.0150 - val_accuracy: 0.9883 - val_loss: 0.0368\nEpoch 10/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 260s 1s/step - accuracy: 0.9956 - loss: 0.0151 - val_accuracy: 0.9822 - val_loss: 0.0507\nEpoch 11/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 204s 1s/step - accuracy: 0.9828 - loss: 0.0615 - val_accuracy: 0.9858 - val_loss: 0.0444\nEpoch 12/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 202s 1s/step - accuracy: 0.9937 - loss: 0.0226 - val_accuracy: 0.9879 - val_loss: 0.0355\n\n\n\nplot_training(combined_history, \"Combined Model Accuracy\")\n\n\n\n\n\n\n\n\nFor the title and text model, the validation accuracy is about 99%, which means this model performed the best out of all three models. The training was also stable, with minimal overfitting. This suggests that combining informaation from both the title and text provides the best results."
  },
  {
    "objectID": "posts/Fake News Detector/index.html#recommendation",
    "href": "posts/Fake News Detector/index.html#recommendation",
    "title": "Fake News Detector",
    "section": "Recommendation",
    "text": "Recommendation\nBased on the results, algorithms should use both title and text of an article when seeking to detect fake news."
  },
  {
    "objectID": "posts/Palmer Penguins Visualization/index.html",
    "href": "posts/Palmer Penguins Visualization/index.html",
    "title": "Palmer Penguins Visualization Post",
    "section": "",
    "text": "For this visualization using Palmer Penguins data set, I decided to create a scatterplot of the penguins’ Culmen Length vs. Culmen Depth. For an added layer of texture, I wanted to separate them by species to get a more distinct correlation."
  },
  {
    "objectID": "posts/Palmer Penguins Visualization/index.html#introduction",
    "href": "posts/Palmer Penguins Visualization/index.html#introduction",
    "title": "Palmer Penguins Visualization Post",
    "section": "",
    "text": "For this visualization using Palmer Penguins data set, I decided to create a scatterplot of the penguins’ Culmen Length vs. Culmen Depth. For an added layer of texture, I wanted to separate them by species to get a more distinct correlation."
  },
  {
    "objectID": "posts/Palmer Penguins Visualization/index.html#importing",
    "href": "posts/Palmer Penguins Visualization/index.html#importing",
    "title": "Palmer Penguins Visualization Post",
    "section": "Importing",
    "text": "Importing\nI started by importing the data set and reading it into the penguins dataframe.\n\nimport pandas as pd\nurl = \"https://raw.githubusercontent.com/pic16b-ucla/24W/main/datasets/palmer_penguins.csv\"\npenguins = pd.read_csv(url)\n\npenguins.head()\n\n\n\n\n\n\n\n\nstudyName\nSample Number\nSpecies\nRegion\nIsland\nStage\nIndividual ID\nClutch Completion\nDate Egg\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSex\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nComments\n\n\n\n\n0\nPAL0708\n1\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN1A1\nYes\n11/11/07\n39.1\n18.7\n181.0\n3750.0\nMALE\nNaN\nNaN\nNot enough blood for isotopes.\n\n\n1\nPAL0708\n2\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN1A2\nYes\n11/11/07\n39.5\n17.4\n186.0\n3800.0\nFEMALE\n8.94956\n-24.69454\nNaN\n\n\n2\nPAL0708\n3\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN2A1\nYes\n11/16/07\n40.3\n18.0\n195.0\n3250.0\nFEMALE\n8.36821\n-25.33302\nNaN\n\n\n3\nPAL0708\n4\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN2A2\nYes\n11/16/07\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nAdult not sampled.\n\n\n4\nPAL0708\n5\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN3A1\nYes\n11/16/07\n36.7\n19.3\n193.0\n3450.0\nFEMALE\n8.76651\n-25.32426\nNaN"
  },
  {
    "objectID": "posts/Palmer Penguins Visualization/index.html#data-cleaning",
    "href": "posts/Palmer Penguins Visualization/index.html#data-cleaning",
    "title": "Palmer Penguins Visualization Post",
    "section": "Data Cleaning",
    "text": "Data Cleaning\nNext is to clean the data by getting rid of all non number values and shortening the species’ names.\n\n# Replacing species names with only their first word\npenguins[\"Species\"] = penguins[\"Species\"].str.split().str.get(0) \n\n# Dropping any NaN values in the relevant columns to our visualization\npenguins.dropna(subset=[\"Culmen Length (mm)\"], inplace=True)\npenguins.dropna(subset=[\"Culmen Depth (mm)\"], inplace=True)\n\npenguins.head()\n\n\n\n\n\n\n\n\nstudyName\nSample Number\nSpecies\nRegion\nIsland\nStage\nIndividual ID\nClutch Completion\nDate Egg\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSex\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nComments\n\n\n\n\n0\nPAL0708\n1\nAdelie\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN1A1\nYes\n11/11/07\n39.1\n18.7\n181.0\n3750.0\nMALE\nNaN\nNaN\nNot enough blood for isotopes.\n\n\n1\nPAL0708\n2\nAdelie\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN1A2\nYes\n11/11/07\n39.5\n17.4\n186.0\n3800.0\nFEMALE\n8.94956\n-24.69454\nNaN\n\n\n2\nPAL0708\n3\nAdelie\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN2A1\nYes\n11/16/07\n40.3\n18.0\n195.0\n3250.0\nFEMALE\n8.36821\n-25.33302\nNaN\n\n\n4\nPAL0708\n5\nAdelie\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN3A1\nYes\n11/16/07\n36.7\n19.3\n193.0\n3450.0\nFEMALE\n8.76651\n-25.32426\nNaN\n\n\n5\nPAL0708\n6\nAdelie\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN3A2\nYes\n11/16/07\n39.3\n20.6\n190.0\n3650.0\nMALE\n8.66496\n-25.29805\nNaN"
  },
  {
    "objectID": "posts/Palmer Penguins Visualization/index.html#plotting-visualization",
    "href": "posts/Palmer Penguins Visualization/index.html#plotting-visualization",
    "title": "Palmer Penguins Visualization Post",
    "section": "Plotting Visualization",
    "text": "Plotting Visualization\nI used Seaborn to plot my visualization. Inputting Culmen Length on the x-axis, Culmen Depth on the y-axis, and Species as the legend indicators.\n\nimport seaborn as sns\n\n# Create scatterplot using seaborn relpot\nplt = sns.relplot(data=penguins, x=\"Culmen Length (mm)\", y=\"Culmen Depth (mm)\", hue=\"Species\")\n\n# Set title and axis labels\nplt.fig.suptitle(\"Culmen Depth vs. Length by Species\")\nplt.set_axis_labels(\"Culmen Length (mm)\", \"Culmen Depth (mm)\")\n\nC:\\Users\\avajt\\anaconda3\\envs\\PIC16B-25W\\Lib\\site-packages\\seaborn\\axisgrid.py:118: UserWarning:\n\nThe figure layout has changed to tight"
  },
  {
    "objectID": "posts/TMDB Web Scrapping/index.html",
    "href": "posts/TMDB Web Scrapping/index.html",
    "title": "TMDB Web Scrapping",
    "section": "",
    "text": "In this assignment, we want to answer the question\n\nWhat movie or TV shows share actors with your favorite movie or show?\n\nTo answer this, I will be using web scraping, which is the process of creating an automated software to extract data from websites. I created a web scraper using scrapy for the TMDB website, where we specifically focus on scraping the movies and tv shows from all shared actors of Harry Potter and the Philosopher’s Stone. Web scraping is"
  },
  {
    "objectID": "posts/TMDB Web Scrapping/index.html#introduction",
    "href": "posts/TMDB Web Scrapping/index.html#introduction",
    "title": "TMDB Web Scrapping",
    "section": "",
    "text": "In this assignment, we want to answer the question\n\nWhat movie or TV shows share actors with your favorite movie or show?\n\nTo answer this, I will be using web scraping, which is the process of creating an automated software to extract data from websites. I created a web scraper using scrapy for the TMDB website, where we specifically focus on scraping the movies and tv shows from all shared actors of Harry Potter and the Philosopher’s Stone. Web scraping is"
  },
  {
    "objectID": "posts/TMDB Web Scrapping/index.html#creating-scrapy-project",
    "href": "posts/TMDB Web Scrapping/index.html#creating-scrapy-project",
    "title": "TMDB Web Scrapping",
    "section": "Creating Scrapy Project",
    "text": "Creating Scrapy Project\nTo initialize the scrapy project on your computer, start by opening your terminal and running this code in your preferred kernal:\nscrapy startproject TMDB_scraper\nThis will create a file called TMDB_scraper in the directory you ran the command. This file will contain many files necessary for running this scrapy project."
  },
  {
    "objectID": "posts/TMDB Web Scrapping/index.html#adjust-settings",
    "href": "posts/TMDB Web Scrapping/index.html#adjust-settings",
    "title": "TMDB Web Scrapping",
    "section": "Adjust Settings",
    "text": "Adjust Settings\nInside the TMDB_scraper file, there is a settings.py document. In here add the line\nCLOSESPIDER_PAGECOUNT = 20\nthis will prevent our scraper from downloading too much data while we are testing our code."
  },
  {
    "objectID": "posts/TMDB Web Scrapping/index.html#creating-tmdbspider-class",
    "href": "posts/TMDB Web Scrapping/index.html#creating-tmdbspider-class",
    "title": "TMDB Web Scrapping",
    "section": "Creating Tmdbspider Class",
    "text": "Creating Tmdbspider Class\nInside our TMDB_scraper file, there is a spiders file which contains a tmdb_spider.py document. In here, add these lines:\nimport scrapy\n\nclass TmdbSpider(scrapy.Spider):\n    name = 'tmdb_spider'\n    def __init__(self, subdir=\"\", *args, **kwargs):\n        self.start_urls = [f\"https://www.themoviedb.org/movie/{subdir}/\"]\nIf you want to choose a different movie than Harry Potter and the Philosopher’s Stone, this will be possible by giving the TMDB subdirectory when we run our completed spider."
  },
  {
    "objectID": "posts/TMDB Web Scrapping/index.html#parse-method",
    "href": "posts/TMDB Web Scrapping/index.html#parse-method",
    "title": "TMDB Web Scrapping",
    "section": "parse method",
    "text": "parse method\nInside of our TmdbSpider(scrapy.Spider) class, we implement the parse(self, response) method. This method will assume we start on the given movie’s TMDB’s subdirectory, then navigate us to the Full Cast & Crew page. Next, it calls the parse_full_credits' method in ourcallbackargument to a yieldedscrapy.Request`.\n    def parse(self, response):\n            \"\"\"Extracts the link to the Full Cast & Crew page dynamically and follows it.\"\"\"\n            cast_relative_url = response.css(\n                    'a[href*=\"/movie/\"][href$=\"/cast\"]::attr(href)').get()\n            if cast_relative_url:\n                    yield scrapy.Request(response.urljoin(cast_relative_url), \n                        callback=self.parse_full_credits)\nWe return nothin in this method, so parse(self, response) is mainly used for navigating to a different page and calling our next method."
  },
  {
    "objectID": "posts/TMDB Web Scrapping/index.html#parse_full_credits-method",
    "href": "posts/TMDB Web Scrapping/index.html#parse_full_credits-method",
    "title": "TMDB Web Scrapping",
    "section": "parse_full_credits method",
    "text": "parse_full_credits method\nInside of our TmdbSpider(scrapy.Spider) class, we implement the parse_full_credits(self, response) method. This method assumes we are on the Full Cast & Crew page, given from the parse(self, response) method we defined. We want to find all the actors listed on this page and yield a scrapy.Request for each one. But, we are not including the crew for this method, which would’ve included different css code. Each yield request calls on parse_actor_page(self, response), so we branch into each actor’s subdirectory on TMDB.\n    def parse_full_credits(self, response):\n            \"\"\"Extracts actor links from the Full Cast & Crew page and follows them.\"\"\"\n            for actor in response.css(\"ol.people.credits a::attr(href)\").getall():\n                    actor_url = response.urljoin(actor)\n                    yield scrapy.Request(actor_url, callback=self.parse_actor_page)\nOnce again, we don’t return anything from this method but are just navigating to another directory within TMDB."
  },
  {
    "objectID": "posts/TMDB Web Scrapping/index.html#parse_actor_page-method",
    "href": "posts/TMDB Web Scrapping/index.html#parse_actor_page-method",
    "title": "TMDB Web Scrapping",
    "section": "parse_actor_page method",
    "text": "parse_actor_page method\nInside of our TmdbSpider(scrapy.Spider) class, we implement the parse_full_credits(self, response) method, which is or last and most complex method. This method assumes we start on the page of an individual actor, which was defined in ’parse_full_credits(self, response). We want this method to yield a dictionary of the form '{\"actor\" : actor_name, \"movie_or_TV_name\" : movie_or_TV_name}, so each dictionary contains two key-values pairs which represent the actor and what movies/TV show they have acted in. If an actor has been in n-movies/TV shows, then there will be n-dictionaries for that specific actor.\nTo achieve this, we use XPath to match the query elements. We are specifically looking in the ‘Acting’ section of the page, and addiing this to a set of unique titles which we transform into our dictionary. Some actors have also done more than acting roles (stunts, features, etc.) which don’t always show up on the first round of retrieval. So we can repreat this process with XPath but looking in the ‘Crew’ section of the page.\n    def parse_actor_page(self, response):\n            \"\"\"Extracts the actor's name and their movies/TV shows from 'Known For' section.\"\"\"\n\n            # Retrieve the actor's name from the title tag in the head section of the page\n            title = response.css('head title::text').get()\n            actor_name = title.replace(' — The Movie Database (TMDB)', '') if title else None\n\n            \"\"\" Extract the movie/TV show titles the actor is associated with \n            by locating the \"Acting\" section \"\"\"\n            acting_section = response.xpath(\n                \"//h3[contains(text(), 'Acting')]/following-sibling::table[1]//bdi//text()\"\n            ).extract()\n\n            # Use a set to eliminate any duplicate movie/TV show titles\n            unique_titles = set(acting_section)\n\n            # Generate the output with the actor's name and the unique movie/TV titles\n            for movie in unique_titles:\n                    yield {\"actor\": actor_name, \"movie_or_TV_name\": movie}\n\n            # Extract crew roles like stunt doubles or other special roles\n            crew_section = response.xpath(\n                \"//h3[contains(text(), 'Crew')]/following-sibling::table[1]//bdi//text()\"\n            ).extract()\n\n            # Include these in the output, ensuring it doesn't miss extra roles\n            for role in set(crew_section):\n                    yield {\"actor\": actor_name, \"movie_or_TV_name\": role}"
  },
  {
    "objectID": "posts/TMDB Web Scrapping/index.html#test-run",
    "href": "posts/TMDB Web Scrapping/index.html#test-run",
    "title": "TMDB Web Scrapping",
    "section": "Test Run",
    "text": "Test Run\nOnce you TmdbSpider(scrapy.Spider) class is fully implemented, go into your terminal and directory where ‘TMDB_scraper’ is located. Then run this command:\nscrapy crawl tmdb_spider -o results.csv -a subdir=671-harry-potter-and-the-philosopher-s-stone\nThis will create a returns.csv in this location, which contains all the dictionaries that were defined by the parse_actor_page(self, response) method. The first column should contain the actor (first dict. value) and the second column should contain the movie/TV show they worked in (second dict. value)."
  },
  {
    "objectID": "posts/TMDB Web Scrapping/index.html#final-run",
    "href": "posts/TMDB Web Scrapping/index.html#final-run",
    "title": "TMDB Web Scrapping",
    "section": "Final Run",
    "text": "Final Run\nOnce you’ve ensured the results.csv' appeared and the code ran with no error, you can now go back into yoursettings.py` and comment out this line:\nCLOSESPIDER_PAGECOUNT = 20\nNow that we can get more data downloaded than before, run this command again in your terminal:\nscrapy crawl tmdb_spider -o results.csv -a subdir=671-harry-potter-and-the-philosopher-s-stone\nwhich runs your spiders and saves the ‘results.csv’."
  },
  {
    "objectID": "posts/TMDB Web Scrapping/index.html#create-sorted-dataframe",
    "href": "posts/TMDB Web Scrapping/index.html#create-sorted-dataframe",
    "title": "TMDB Web Scrapping",
    "section": "Create Sorted Dataframe",
    "text": "Create Sorted Dataframe\nNext is to create a sorted list to display our results.csv. For this sorted dataframe, I want to sort it by how many movies/TV shows share the same actors based on Harry Potter and the Philosopher’s Stone. We want to have a column contain the movie’s/TV show’s name and another column contain the number of shared actors.\nStart by importing your necessary packages, then use pandas to read in you results.csv into a dataframe. Next, we count the number of shared actors per movie/Tv show by counting the how many times each movie/TV show appears. Based on this index counting, we can relable our columns to describe our number of shared actors.\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the scraped data\ndf = pd.read_csv(\"results.csv\")\n\n# Count the number of shared actors per movie/TV show\nshared_movies = df[\"movie_or_TV_name\"].value_counts().reset_index()\nshared_movies.columns = [\"movie_or_TV_name\", \"num_shared_actors\"]\n\n# Display the top 10 movies/shows that share actors\nprint(shared_movies.head(10))\n\n                                    movie_or_TV_name  num_shared_actors\n0           Harry Potter and the Philosopher's Stone                 32\n1            Harry Potter and the Chamber of Secrets                 28\n2           Harry Potter and the Prisoner of Azkaban                 24\n3          Harry Potter and the Order of the Phoenix                 24\n4       Harry Potter and the Deathly Hallows: Part 2                 21\n5             Harry Potter and the Half-Blood Prince                 19\n6       Harry Potter and the Deathly Hallows: Part 1                 18\n7                Harry Potter and the Goblet of Fire                 18\n8  Harry Potter 20th Anniversary: Return to Hogwarts                 13\n9                          Who Do You Think You Are?                  9\n\n\nNow that we have the sorted dataframe shared_movies with our shared actors for the top 10 movies, we can visualize this into a bargraph using matlab.\n\n# Plot the top 10 shared movies/shows\nplt.figure(figsize=(5, 3))\nplt.barh(shared_movies[\"movie_or_TV_name\"][:10], shared_movies[\"num_shared_actors\"][:10], \n    color=\"skyblue\")\nplt.xlabel(\"Number of Shared Actors\")\nplt.ylabel(\"Movie / TV Show\")\nplt.title(\"Top Movies\\TV Shows with Shared Actors\")\nplt.gca().invert_yaxis()  # Invert for better readability\nplt.show()\n\n\n\n\n\n\n\n\nWe have succesfully scrapped information containing each movie/TV show each actor who worked on Harry Potter and the Philosopher’s Stone from the TMDB website by creating three different parsing methods, and visualized how many movies contain shared actors."
  }
]